# -*- coding: utf-8 -*-
"""
RAG Orchestrator (Arabic-only, generalized)
- Arabic normalization, diacritics removal, and digit unification
- Page-aware fallback (from cited pages, then anchors, then global)
- Generic regex hunter for numbers/times/days/%/durations (Arabic-centric)
- Signal-based sentence ranking (numeric/time/day signals + Arabic policy cues)
- Arabic cleanup + heading/TOC/boilerplate suppression
- Short, clipped bullets with pagination
- Strict guard: numeric/time questions must include numeric/time tokens; otherwise say "not specified" from text

Usage (typical):
python NewRag_arabic_only_patched.py --chunks Data_pdf_clean_chunks.jsonl --sanity --device cuda --use-4bit --no-llm \
  --regex-hunt --hourlines-only --max-bullets 5 --bullet-max-chars 120 --paginate-chars 600 --out-dir runs
"""

import os, sys, re, json, time, argparse, logging
from datetime import datetime
from collections import defaultdict
from types import SimpleNamespace

# Quiet noisy libs
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
os.environ.setdefault("TRANSFORMERS_VERBOSITY", "error")
os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")

try:
    import torch  # optional
except Exception:
    torch = None

# Your retriever module
import retrival_model as RET

# ---------------- Utilities for dict-or-object chunks ----------------
def _get_attr_or_key(obj, key, default=None):
    if isinstance(obj, dict):
        return obj.get(key, default)
    val = getattr(obj, key, None)
    if val is not None: return val
    for nest in ("meta", "metadata", "__dict__"):
        container = getattr(obj, nest, None)
        if isinstance(container, dict) and key in container:
            return container.get(key)
    return default

def _first_non_empty(*vals):
    for v in vals:
        if v is None: continue
        s = str(v)
        if s.strip(): return s
    return ""

def _first_page_like(obj):
    candidates = (
        _get_attr_or_key(obj, "page"),
        _get_attr_or_key(obj, "page_number"),
        _get_attr_or_key(obj, "page_num"),
        _get_attr_or_key(obj, "page_idx"),
        _get_attr_or_key(obj, "pageno"),
        _get_attr_or_key(obj, "page_start"),
    )
    for c in candidates:
        if c is None or str(c).strip() == "": continue
        try: return int(str(c).strip())
        except Exception: pass
    return None

# ---------------- Global page index ----------------
CHUNKS_BY_PAGE = {}  # {int page: full concatenated text}

def _build_page_text_index(chunks):
    pages = defaultdict(list)
    for ch in chunks:
        txt = _first_non_empty(
            _get_attr_or_key(ch, "text"),
            _get_attr_or_key(ch, "content"),
            _get_attr_or_key(ch, "chunk"),
            _get_attr_or_key(ch, "body"),
            "",
        )
        if isinstance(txt, bytes):
            try: txt = txt.decode("utf-8", "ignore")
            except Exception: txt = ""
        pg = _first_page_like(ch)
        if pg is None:
            src = _get_attr_or_key(ch, "source") or _get_attr_or_key(ch, "doc_source") or ""
            m = re.search(r"(?:[Pp]age|(?:Ø§Ù„)?ØµÙØ­Ø©)\s+(\d+)", str(src))
            if m:
                try: pg = int(m.group(1))
                except Exception: pg = None
        if pg is not None and txt:
            pages[int(pg)].append(str(txt))
    return {p: "\n".join(v) for p, v in pages.items()}

# ---------------- Logging ----------------
def setup_logger(log_path: str):
    logger = logging.getLogger("rag_orchestrator")
    logger.setLevel(logging.INFO)
    ch = logging.StreamHandler(sys.stdout); ch.setLevel(logging.INFO)
    ch.setFormatter(logging.Formatter("%(levelname)s:%(name)s:%(message)s"))
    fh = logging.FileHandler(log_path, encoding="utf-8"); fh.setLevel(logging.INFO)
    fh.setFormatter(logging.Formatter("%(asctime)s %(levelname)s:%(name)s:%(message)s"))
    logger.handlers = []; logger.addHandler(ch); logger.addHandler(fh)
    return logger

LOG = logging.getLogger("rag_orchestrator")  # reset in main()

# ---------------- Sanity Prompts (default) ----------------
DEFAULT_SANITY_PROMPTS = [
    "Ù…Ø§ Ù‡ÙŠ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù… Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ù…Ù† ÙˆØ¥Ù„Ù‰ØŸ",
    "Ù‡Ù„ ÙŠÙˆØ¬Ø¯ Ù…Ø±ÙˆÙ†Ø© ÙÙŠ Ø§Ù„Ø­Ø¶ÙˆØ± ÙˆØ§Ù„Ø§Ù†ØµØ±Ø§ÙØŸ ÙˆÙƒÙŠÙ ØªÙØ­Ø³Ø¨ Ø¯Ù‚Ø§Ø¦Ù‚ Ø§Ù„ØªØ£Ø®ÙŠØ±ØŸ",
    "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ±Ø§Ø­Ø© Ø®Ù„Ø§Ù„ Ø§Ù„Ø¯ÙˆØ§Ù…ØŸ ÙˆÙƒÙ… Ù…Ø¯ØªÙ‡Ø§ØŸ",
    "Ù…Ø§ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø´Ù‡Ø± Ø±Ù…Ø¶Ø§Ù†ØŸ ÙˆÙ‡Ù„ ØªØªØºÙŠØ±ØŸ",
    "Ù…Ø§ Ø£ÙŠØ§Ù… Ø§Ù„Ø¯ÙˆØ§Ù… Ø§Ù„Ø±Ø³Ù…ÙŠØŸ ÙˆÙ‡Ù„ Ø§Ù„Ø³Ø¨Øª ÙŠÙˆÙ… Ø¹Ù…Ù„ØŸ",
    "ÙƒÙŠÙ ÙŠÙØ­ØªØ³Ø¨ Ø§Ù„Ø£Ø¬Ø± Ø¹Ù† Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø£ÙŠØ§Ù… Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©ØŸ",
    "Ù…Ø§ Ø§Ù„ØªØ¹ÙˆÙŠØ¶ Ø¹Ù†Ø¯ Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„Ø±Ø³Ù…ÙŠØ©ØŸ",
    "Ù‡Ù„ ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ù…Ø³Ø¨Ù‚Ø©ØŸ ÙˆÙ…Ù† ÙŠØ¹ØªÙ…Ø¯Ù‡Ø§ØŸ",
    "ÙƒÙ… Ù…Ø¯Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø³Ù†ÙˆÙŠØ© Ù„Ù…ÙˆØ¸Ù Ø¬Ø¯ÙŠØ¯ØŸ ÙˆÙ…ØªÙ‰ ØªØ²ÙŠØ¯ØŸ",
    "Ù‡Ù„ ØªÙØ±Ø­Ù‘Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©ØŸ ÙˆÙ…Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ØŸ",
    "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø·Ø§Ø±Ø¦Ø©ØŸ ÙˆÙƒÙŠÙ Ø£Ø·Ù„Ø¨Ù‡Ø§ØŸ",
    "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ù…Ø±Ø¶ÙŠØ©ØŸ ÙˆØ¹Ø¯Ø¯ Ø£ÙŠØ§Ù…Ù‡Ø§ØŸ ÙˆÙ‡Ù„ ÙŠÙ„Ø²Ù… ØªÙ‚Ø±ÙŠØ± Ø·Ø¨ÙŠØŸ",
    "ÙƒÙ… Ù…Ø¯Ø© Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø£Ù…ÙˆÙ…Ø©ØŸ ÙˆÙ‡Ù„ ÙŠÙ…ÙƒÙ† Ø£Ø®Ø° Ø¬Ø²Ø¡ Ù‚Ø¨Ù„ Ø§Ù„ÙˆÙ„Ø§Ø¯Ø©ØŸ",
    "Ù…Ø§ Ù‡ÙŠ Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø­Ø¯Ø§Ø¯ØŸ Ù„Ù…Ù† ØªÙÙ…Ù†Ø­ ÙˆÙƒÙ… Ù…Ø¯ØªÙ‡Ø§ØŸ",
    "Ù…ØªÙ‰ ÙŠØªÙ… ØµØ±Ù Ø§Ù„Ø±ÙˆØ§ØªØ¨ Ø´Ù‡Ø±ÙŠÙ‹Ø§ØŸ",
    "Ù…Ø§ Ù‡Ùˆ Ø¨Ø¯Ù„ Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§ØªØŸ ÙˆÙ‡Ù„ ÙŠØ´Ù…Ù„ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù…Ù† Ø§Ù„Ù…Ù†Ø²Ù„ Ù„Ù„Ø¹Ù…Ù„ØŸ ÙˆÙƒÙŠÙ ÙŠÙØµØ±ÙØŸ",
    "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø³Ù„Ù Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§ØªØ¨ØŸ ÙˆÙ…Ø§ Ø´Ø±ÙˆØ·Ù‡Ø§ØŸ",
    "Ù…Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù†Ø«Ø±ÙŠØ§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©ØŸ ÙˆÙƒÙŠÙ ØªØªÙ… Ø§Ù„ØªØ³ÙˆÙŠØ© ÙˆØ§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŸ",
    "Ù…Ø§ Ø³Ù‚Ù Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø°ÙŠ ÙŠØ³ØªÙ„Ø²Ù… Ø«Ù„Ø§Ø«Ø© Ø¹Ø±ÙˆØ¶ Ø£Ø³Ø¹Ø§Ø±ØŸ",
    "Ù…Ø§ Ø¶ÙˆØ§Ø¨Ø· ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ù…ØµØ§Ù„Ø­ ÙÙŠ Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§ØªØŸ",
    "Ù…Ø§ Ø­Ø¯ÙˆØ¯ Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù‡Ø¯Ø§ÙŠØ§ ÙˆØ§Ù„Ø¶ÙŠØ§ÙØ©ØŸ ÙˆÙ…ØªÙ‰ ÙŠØ¬Ø¨ Ø§Ù„Ø¥Ø¨Ù„Ø§ØºØŸ",
    "ÙƒÙŠÙ Ø£Ø³ØªÙ„Ù… Ø¹Ù‡Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø©ØŸ ÙˆÙ…Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ØŸ",
    "ÙƒÙŠÙ Ø£Ø³Ù„Ù‘Ù… Ø§Ù„Ø¹Ù‡Ø¯Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªÙ‚Ø§Ù„Ø© Ø£Ùˆ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ØŸ",
    "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨ÙØ¹Ø¯/Ù…Ù† Ø§Ù„Ù…Ù†Ø²Ù„ØŸ ÙˆÙƒÙŠÙ ÙŠØªÙ… Ø§Ø¹ØªÙ…Ø§Ø¯Ù‡ØŸ",
    "ÙƒÙŠÙ Ø£Ù‚Ø¯Ù‘Ù… Ø¥Ø°Ù† Ù…ØºØ§Ø¯Ø±Ø© Ø³Ø§Ø¹ÙŠØ©ØŸ ÙˆÙ…Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ø§Ù„Ø´Ù‡Ø±ÙŠØŸ",
    "Ù…ØªÙ‰ ÙŠØªÙ… ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø³Ù†ÙˆÙŠØŸ ÙˆÙ…Ø§ Ù…Ø¹Ø§ÙŠÙŠØ±Ù‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŸ",
    "Ù…Ø§ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¥Ù†Ø°Ø§Ø± ÙˆØ§Ù„ØªØ¯Ø±Ù‘Ø¬ Ø§Ù„ØªØ£Ø¯ÙŠØ¨ÙŠ Ù„Ù„Ù…Ø®Ø§Ù„ÙØ§ØªØŸ",
    "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø³Ø±ÙŠØ© ÙˆØ­Ù…Ø§ÙŠØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŸ",
    "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ù‡Ù†ÙŠ ÙˆÙ…ÙƒØ§ÙØ­Ø© Ø§Ù„ØªØ­Ø±Ø´ØŸ",
    "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ù…ÙŠØ§ÙˆÙ…Ø§Øª/Ø¨Ø¯Ù„ Ø³ÙØ±ØŸ ÙˆÙƒÙŠÙ ØªÙØµØ±Ù",
]

# ---------------- Arabic-only normalization helpers ----------------
_ARABIC_DIACRITICS = dict.fromkeys(map(ord, "ÙÙ‹ÙÙŒÙÙÙ’Ù‘Ù€"), None)  # tashkeel + tatweel

def _norm_arabic_letters(s: str) -> str:
    """Unify Arabic variants and remove diacritics for robust matching."""
    if not s: return ""
    s = s.translate(_ARABIC_DIACRITICS)
    s = s.replace("Ø£","Ø§").replace("Ø¥","Ø§").replace("Ø¢","Ø§")
    s = s.replace("Ù‰","ÙŠ").replace("Ø©","Ù‡")
    s = s.replace("Ø¤","Ùˆ").replace("Ø¦","ÙŠ")
    return s

def _normalize_text_ar(s: str) -> str:
    s = _to_western_digits(s or "").strip()
    s = _norm_arabic_letters(s)
    s = re.sub(r"\s+", " ", s)
    return s

# ---------------- Arabic helpers / checks ----------------
_HEADING_PATTERNS = [
    r"^\s*Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©\s*:?$",
    r"^\s*Ø§Ù„Ø®Ù„Ø§ØµØ©\s*:?\s*$",
    r"^\s*Ø§Ù„Ù…Ù„Ø®Øµ\s*:?\s*$",
    r"^\s*Summary\s*:?\s*$",
    r"^\s*Answer\s*:?\s*$",
]
_AR_DAYS = ["Ø§Ù„Ø£Ø­Ø¯","Ø§Ù„Ø¥Ø«Ù†ÙŠÙ†","Ø§Ù„Ø§Ø«Ù†ÙŠÙ†","Ø§Ù„Ø«Ù„Ø§Ø«Ø§Ø¡","Ø§Ù„Ø£Ø±Ø¨Ø¹Ø§Ø¡","Ø§Ù„Ø®Ù…ÙŠØ³","Ø§Ù„Ø¬Ù…Ø¹Ø©","Ø§Ù„Ø³Ø¨Øª",
            "Ø§Ù„Ø§Ø­Ø¯","Ø§Ù„Ø§Ø±Ø¨Ø¹Ø§Ø¡"]

_TIME_PATTERNS = [
    r"\b\d{1,2}:\d{2}\b",                     # 8:30
    r"\b\d{1,2}[:Ù«]\d{2}\b",                  # 8Ù«30
    r"\b\d{1,2}\s*[-â€“]\s*\d{1,2}\b",          # 8-5
    r"\b\d{1,2}\s*(?:Ø¥Ù„Ù‰|Ø­ØªÙ‰)\s*\d{1,2}\b",   # 8 Ø¥Ù„Ù‰ 5
    r"\b\d{1,2}\s*(?:Øµ|Ù…)\b",                 # 8 Øµ/5 Ù…
]
_PERCENT_RX = re.compile(r"\b\d{1,3}\s*[%Ùª]\b")
_DURATION_RX = re.compile(r"\b\d{1,2}\s*(?:Ø¯Ù‚ÙŠÙ‚Ø©|Ø¯Ù‚Ø§Ø¦Ù‚|Ø³Ø§Ø¹Ø©|Ø³Ø§Ø¹Ø§Øª|ÙŠÙˆÙ…|Ø£ÙŠØ§Ù…)\b")
_RANGE_RX = re.compile(r"\b\d{1,2}\s*[-â€“]\s*\d{1,2}\b")
_NUMERICISH = re.compile(r"(\d|[%Ùª])")
_SECTION_HEAVY = re.compile(r"(?:\d+\.){2,}\d+")  # lines like 3.1.5.7 (TOC-ish)

_ARABIC_DIGITS = str.maketrans("Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©", "0123456789")

def _to_western_digits(s): return (s or "").translate(_ARABIC_DIGITS)

def _strip_mojibake(s):
    if not s:
        return ""
    return (
        s.replace("ï»¿", "")   # BOM
         .replace("ï¿½", "")   # replacement char
         .replace("ï¿½", "")        # literal replacement char
         .replace("Â ", " ")    # non-breaking space
    )

def _arabic_ratio(s):
    if not s: return 1.0
    letters = re.findall(r"\w", s, flags=re.UNICODE)
    if not letters: return 1.0
    arabic = re.findall(r"[Ø¡-ÙŠ]", s)
    return (len(arabic) / max(1, len(letters)))

def _purge_non_arabic_lines(s, min_ratio=0.66):
    if not s: return s
    keep = []
    for line in s.splitlines():
        ln = line.strip()
        if not ln: continue
        # keep numeric-only or time lines even if not Arabic-heavy
        t = _to_western_digits(ln)
        if any(re.search(p, t) for p in _TIME_PATTERNS) or _PERCENT_RX.search(t) or _DURATION_RX.search(t):
            keep.append(ln); continue
        ratio = _arabic_ratio(ln)
        if ratio >= min_ratio: keep.append(ln)
    return "\n".join(keep)

def _has_times_or_days(txt):
    if not txt: return False
    t = _to_western_digits(txt)
    if any(day in t for day in _AR_DAYS): return True
    if any(re.search(p, t) for p in _TIME_PATTERNS): return True
    if _DURATION_RX.search(t): return True
    if _PERCENT_RX.search(t): return True
    return False

def _sentences(txt):
    if not txt: return []
    lines = [l.strip() for l in txt.splitlines() if l.strip()]
    keep = []
    for l in lines:
        if any(re.match(p, l) for p in _HEADING_PATTERNS): continue
        keep.append(l)
    txt2 = " ".join(keep)
    parts = re.split(r"(?<=[\.\!\ØŸ])\s+|[\n\r]+|[â€¢\-â€“]\s+", txt2)
    parts = [p.strip(" -â€“â€¢\t") for p in parts if p and len(p.strip()) > 0]
    merged = []
    for p in parts:
        if merged and len(p) < 10:
            merged[-1] = merged[-1] + " " + p
        else:
            merged.append(p)
    return merged

def _clean_text(txt):
    if not txt: return ""
    txt = _strip_mojibake(txt)
    txt = re.sub(r"^```.*?$", "", txt, flags=re.M | re.S)
    lines = [l.strip() for l in txt.splitlines() if l.strip()]
    keep = []
    for l in lines:
        if any(re.match(p, l) for p in _HEADING_PATTERNS): continue
        # drop TOC-ish and boilerplate
        if "ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª" in l or "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø­ÙÙˆØ¸Ø©" in l: continue
        if _SECTION_HEAVY.search(l): continue
        keep.append(l)
    txt = " ".join(keep).strip()
    txt = re.sub(r"\s+", " ", txt).strip()
    return txt

def _paginate_text(text, max_chars=600):
    text = text.strip()
    if len(text) <= max_chars: return [text]
    parts, cur, count = [], [], 0
    for line in text.splitlines():
        if count + len(line) + 1 > max_chars:
            parts.append("\n".join(cur).strip())
            cur, count = [line], len(line)
        else:
            cur.append(line); count += len(line) + 1
    if cur: parts.append("\n".join(cur).strip())
    return parts

def _split_answer(answer_text):
    if not answer_text: return "", ""
    parts = re.split(r"\n(?=Sources:|Ø§Ù„Ù…ØµØ§Ø¯Ø±:)", answer_text, maxsplit=1)
    body = parts[0].strip(); sources = parts[1].strip() if len(parts) > 1 else ""
    return body, sources

# ---------------- Q intent + keywords ----------------
POLICY_VERBS = ("ÙŠØ¬Ø¨","ÙŠÙ„Ø²Ù…","ÙŠÙÙ…Ù†Ø¹","ÙŠÙØ­Ø¸Ø±","ÙŠØªØ¹ÙŠÙ‘Ù†","Ù„Ø§ ÙŠØ¬ÙˆØ²","ÙŠØ­Ù‚","ØªÙƒÙˆÙ†","ÙŠØªÙ…","ÙˆÙÙ‚Ø§Ù‹","Ø­Ø³Ø¨")

def _norm_tokens(s):
    s = _to_western_digits(s or "")
    s = re.sub(r"[^\w\s%Ùª:â€“\-]+"," ", s, flags=re.UNICODE)
    return [t for t in s.split() if t.strip()]

def _question_keywords(q):
    toks = _norm_tokens(q)
    # add simple Arabic stems
    extras = []
    if "Ø³Ø§Ø¹Ø§Øª" in q or "Ø¯ÙˆØ§Ù…" in q: extras += ["Ø³Ø§Ø¹Ø§Øª","Ø¯ÙˆØ§Ù…","Ù…Ù†","Ø¥Ù„Ù‰","Ø­ØªÙ‰","Ø§ÙˆÙ‚Ø§Øª"]
    if "Ø±Ù…Ø¶Ø§Ù†" in q or "Ø§Ù„ØµÙˆÙ…" in q: extras += ["Ø±Ù…Ø¶Ø§Ù†","Ø§Ù„ØµÙˆÙ…"]
    if "Ø¥Ø¶Ø§Ù" in q or "Ø§Ø¶Ø§ÙÙŠ" in q: extras += ["Ø¥Ø¶Ø§ÙÙŠØ©","Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø§Ø¶Ø§ÙÙŠ","Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø§Ø¶Ø§ÙÙŠÙ‡","Ø§Ø¬Ø±"]
    if "Ø¹Ø·Ù„" in q or "Ø¹Ø·Ù„Ø©" in q: extras += ["Ø§Ù„Ø¹Ø·Ù„","Ø§Ù„Ø±Ø³Ù…ÙŠØ©","Ø¹Ø·Ù„Ø©","Ù†Ù‡Ø§ÙŠØ©","Ø£Ø³Ø¨ÙˆØ¹"]
    if "Ø§Ø³ØªØ±Ø§Ø­Ø©" in q or "Ø±Ø§Ø­Ø©" in q: extras += ["Ø§Ø³ØªØ±Ø§Ø­Ø©","Ø±Ø§Ø­Ø©","Ù…Ø¯Ø©","Ù…Ø¯ØªÙ‡Ø§"]
    if "Ù…ØºØ§Ø¯Ø±Ø©" in q or "Ø§Ø°Ù†" in q: extras += ["Ù…ØºØ§Ø¯Ø±Ø©","Ø³Ø§Ø¹ÙŠØ©","Ø§Ù„Ø­Ø¯","Ø§Ù„Ø£Ù‚ØµÙ‰","Ø´Ù‡Ø±ÙŠ","Ø§Ø°Ù†"]
    if "Ø¥Ø¬Ø§Ø²Ø©" in q or "Ø§Ø¬Ø§Ø²Ù‡" in q: extras += ["Ø¥Ø¬Ø§Ø²Ø©","Ø£ÙŠØ§Ù…","Ù…Ø¯Ø©","Ø³Ù†Ùˆ"]
    if "Ø³Ù‚Ù" in q or "Ø¹Ø±ÙˆØ¶" in q: extras += ["Ø³Ù‚Ù","Ø¹Ø±ÙˆØ¶","Ø£Ø³Ø¹Ø§Ø±","Ø«Ù„Ø§Ø«Ø©"]
    if "ØªØ¶Ø§Ø±Ø¨" in q: extras += ["ØªØ¶Ø§Ø±Ø¨","Ù…ØµØ§Ù„Ø­","Ø§Ù„Ù‡Ø¯Ø§ÙŠØ§"]
    return list(dict.fromkeys(toks + extras))

def _expects_numerics(q):
    q = q or ""
    cues = ("ÙƒÙ…","Ù…Ø¯Ø©","Ù…Ø§ Ø§Ù„Ø­Ø¯","Ø§Ù„Ø­Ø¯","Ù…Ù† ÙˆØ¥Ù„Ù‰","Ù…Ù† ÙˆØ§Ù„Ù‰","Ù…Ù† Ø¥Ù„Ù‰","Ù…ØªÙ‰","Ø§Ù„Ù†Ø³Ø¨Ø©","%","Ùª","Ø³Ø§Ø¹Ø§Øª","Ø¯Ù‚Ø§Ø¦Ù‚","ÙŠÙˆÙ…","Ø£ÙŠØ§Ù…","Ø£Ø¬Ø±","ØªØ¹ÙˆÙŠØ¶","Ø³Ù‚Ù","Ø¨Ø¯Ù„","Ù…ÙŠØ§ÙˆÙ…Ø§Øª","Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰","Ø´Ù‡Ø±ÙŠ")
    return any(c in q for c in cues)

# ---------------- Sources â†’ pages helpers ----------------
def _parse_pages_from_sources(sources_text):
    if not sources_text: return []
    s = _to_western_digits(sources_text)
    pages = set()
    for m in re.findall(r"(?:\b[Pp]age\b|(?:Ø§Ù„)?ØµÙØ­Ø©)\s+(\d+)", s):
        try: pages.add(int(m))
        except Exception: pass
    return sorted(pages)

def _page_ctx_from_pages(pages, max_chars=3500):
    if not pages: return ""
    buf, total = [], 0
    for p in pages:
        txt = CHUNKS_BY_PAGE.get(p, "")
        if not txt: continue
        t = _clean_text(txt)
        if total + len(t) > max_chars:
            remaining = max_chars - total
            if remaining > 200:
                buf.append(t[:remaining]); total = max_chars; break
        else:
            buf.append(t); total += len(t)
    return " ".join(buf).strip()

# ---------------- Generic regex hunter (question-driven) ----------------
GENERIC_RXS = [
    re.compile(r"\b\d{1,2}:\d{2}\b"),                     # 08:30
    re.compile(r"\b\d{1,2}[:Ù«]\d{2}\b"),                  # 08Ù«30
    re.compile(r"\b\d{1,2}\s*(?:Øµ|Ù…)\b"),                 # 8 Øµ
    re.compile(r"\b\d{1,2}\s*(?:Ø¥Ù„Ù‰|Ø­ØªÙ‰|-\s*|â€“\s*)\s*\d{1,2}\b"),  # 8 Ø¥Ù„Ù‰ 3 / 8-3
    re.compile(r"\b\d{1,3}\s*[%Ùª]\b"),                    # 150% / Ù¡Ù¥Ù Ùª
    re.compile(r"\b\d{1,2}\s*(?:Ø¯Ù‚ÙŠÙ‚Ø©|Ø¯Ù‚Ø§Ø¦Ù‚|Ø³Ø§Ø¹Ø©|Ø³Ø§Ø¹Ø§Øª|ÙŠÙˆÙ…|Ø£ÙŠØ§Ù…)\b"),
    re.compile(r"\b\d+\b"),                               # plain numbers as last resort
]

def _line_has_generic_numeric(t):
    T = _to_western_digits(t)
    for rx in GENERIC_RXS:
        if rx.search(T): return True
    if any(d in T for d in _AR_DAYS): return True
    return False

def _regex_hunt_generic(text, q_kws):
    if not text: return []
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    hits = []
    for l in lines:
        L = _clean_text(l)
        if not L: continue
        if _SECTION_HEAVY.search(L): continue
        if "ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª" in L or "Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ù…Ø­ÙÙˆØ¸Ø©" in L: continue
        # --- Scoring (ARABIC-ONLY, GENERALIZED) ---
        score = 0
        T = _normalize_text_ar(L)
        # 1) numeric/time/day detectors
        if _line_has_generic_numeric(L):
            score += 3
        # 2) Arabic domain cues
        DOMAIN_KWS_AR = [
            "Ù…ÙˆØ§ÙÙ‚Ù‡", "Ù…ÙˆØ§ÙÙ‚Ù‡ Ù…Ø³Ø¨Ù‚Ù‡", "Ù…ÙˆØ§ÙÙ‚Ù‡ Ø®Ø·ÙŠÙ‡", "Ø§Ø°Ù†", "Ø§Ø°Ù† Ø®Ø·ÙŠ",
            "ØªØ¹ÙˆÙŠØ¶", "ÙŠØ­ØªØ³Ø¨", "ÙŠØ¹ØªÙ…Ø¯", "ÙŠØªØ·Ù„Ø¨", "ÙŠÙ„Ø²Ù…", "ÙŠØ¬Ø¨", "Ù„Ø§ ÙŠØ¬ÙˆØ²", "ÙŠØ­Ù‚", "ÙˆÙÙ‚", "Ø­Ø³Ø¨",
            "Ø¯ÙˆØ§Ù…", "Ø³Ø§Ø¹Ø§Øª", "Ø§Ø¬Ø§Ø²Ù‡", "Ø§Ø¬Ø§Ø²Ø§Øª", "Ø¹Ø·Ù„Ù‡", "Ø§Ù„Ø¹Ø·Ù„", "Ù…ØºØ§Ø¯Ø±Ù‡", "Ø§Ø³ØªØ±Ø§Ø­Ù‡",
            "Ø¨Ø¯Ù„", "Ø¨Ø¯Ù„ Ù…ÙˆØ§ØµÙ„Ø§Øª", "Ù…ÙŠØ§ÙˆÙ…Ø§Øª", "Ø¨Ø¯Ù„ Ø³ÙØ±", "Ø±ÙˆØ§ØªØ¨", "ØµØ±Ù", "Ù…ÙˆØ¹Ø¯", "Ø§Ù„Ø­Ø¶ÙˆØ±", "Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠÙ‡",
            "Ø¹Ø±ÙˆØ¶ Ø§Ø³Ø¹Ø§Ø±", "Ø¹Ø±Ø¶ Ø³Ø¹Ø±", "Ø¹Ø±ÙˆØ¶", "Ù…Ù†Ø§Ù‚ØµØ©", "ØªÙˆØ±ÙŠØ¯", "ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ù…ØµØ§Ù„Ø­",
            # currencies / money cues
            "Ø´ÙŠÙƒÙ„", "Ø´ÙˆØ§Ù‚Ù„", "Ø¯ÙŠÙ†Ø§Ø±", "Ø¯ÙˆÙ„Ø§Ø±", "â‚ª"
        ]
        domain_kws = set(DOMAIN_KWS_AR + (q_kws or []))
        if _line_has_generic_numeric(L) and any(kw in T for kw in domain_kws):
            score += 2
        # 3) penalize orphan numbers
        if re.search(r"\b\d+\b", T) and not any(k in T for k in domain_kws):
            score -= 2
        # 4) rule/policy bonus
        POLICY_STEMS_AR = ["ÙŠØ¬Ø¨","ÙŠÙ„Ø²Ù…","Ù„Ø§ ÙŠØ¬ÙˆØ²","ÙŠØ­Ù‚","ÙˆÙÙ‚","Ø­Ø³Ø¨","Ø¹Ù„Ù‰","ÙŠÙƒÙˆÙ†","ØªÙƒÙˆÙ†"]
        if any(stem in T for stem in POLICY_STEMS_AR):
            score += 1
        # 5) concise statement bonus
        ln = len(T)
        if 20 <= ln <= 160:
            score += 1
        if score > 0:
            hits.append((score, L))
    # sort by score desc, length asc
    hits.sort(key=lambda x: (-x[0], len(x[1])))
    return [h[1] for h in hits]

# ---------------- Bullet formatting ----------------
def _clip(s: str, n: int) -> str:
    s = s.strip()
    if n and len(s) > n:
        return (s[:max(1, n-1)]).rstrip() + "â€¦"
    return s

def _as_bullets_clipped(sents, limit=5, max_chars=120):
    sents = sents[:max(1, limit)]
    sents = [_clip(s, max_chars) for s in sents]
    # dedupe
    out, seen = [], set()
    for s in sents:
        if s and s not in seen:
            seen.add(s); out.append(f"â€¢ {s}")
    return "\n".join(out)

def _filter_hourlines(sents):
    out = []
    for s in sents:
        t = _to_western_digits(s)
        if _has_times_or_days(t):
            out.append(s)
    return out

def _bullets_for_display(text: str, question: str, intent: str, cfg):
    sents = _sentences(text)
    if cfg.hourlines_only and (_is_hours_like(question, intent) or _expects_numerics(question)):
        sents = _filter_hourlines(sents)
    if not sents:
        sents = _sentences(text)
    return _as_bullets_clipped(sents, limit=cfg.max_bullets, max_chars=cfg.bullet_max_chars)

# ---------------- Intent & anchoring (Arabic-only) ----------------
INTENT_ALIASES_AR = {
    "work_hours": ["Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¹Ù…Ù„","Ø§ÙˆÙ‚Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù…","Ø§Ù„Ø¯ÙˆØ§Ù… Ø§Ù„Ø±Ø³Ù…ÙŠ","Ù†Ø¸Ø§Ù… Ø§Ù„Ø¯ÙˆØ§Ù…"],
    "ramadan_hours": ["Ø±Ù…Ø¶Ø§Ù†","Ø³Ø§Ø¹Ø§Øª Ø±Ù…Ø¶Ø§Ù†","Ø¯ÙˆØ§Ù… Ø±Ù…Ø¶Ø§Ù†","Ø§Ù„ØµÙˆÙ…"],
    "overtime": ["Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø§Ø¶Ø§ÙÙŠ","Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø§Ø¶Ø§ÙÙŠÙ‡","Ø§Ø¶Ø§ÙÙŠ"],
    "holidays_work": ["Ø§Ù„Ø¹Ø·Ù„","Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„Ø±Ø³Ù…ÙŠÙ‡","Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¹Ø·Ù„"],
    "timesheets": ["Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠÙ‡","Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±","Ø§Ù„Ø­Ø¶ÙˆØ±","Ø§Ù„Ø§Ù†ØµØ±Ø§Ù"],
    "payroll": ["Ø§Ù„Ø±ÙˆØ§ØªØ¨","ØµØ±Ù Ø§Ù„Ø±ÙˆØ§ØªØ¨","Ø§Ù„Ø§Ø¬Ø±"],
    "transport": ["Ø¨Ø¯Ù„ Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª","Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª","Ù†Ù‚Ù„"],
    "hourly_leave": ["Ù…ØºØ§Ø¯Ø±Ù‡ Ø³Ø§Ø¹ÙŠÙ‡","Ø§Ø°Ù† Ù…ØºØ§Ø¯Ø±Ù‡"],
    "annual_leave": ["Ø§Ù„Ø§Ø¬Ø§Ø²Ù‡ Ø§Ù„Ø³Ù†ÙˆÙŠÙ‡","Ø³Ù†ÙˆÙŠÙ‡"],
    "sick_leave": ["Ø§Ù„Ø§Ø¬Ø§Ø²Ù‡ Ø§Ù„Ù…Ø±Ø¶ÙŠÙ‡","Ù…Ø±Ø¶ÙŠÙ‡"],
    "carryover": ["ØªØ±Ø­ÙŠÙ„ Ø§Ù„Ø§Ø¬Ø§Ø²Ø§Øª","ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ù‡","Ø±ØµÙŠØ¯ Ø§Ù„Ø§Ø¬Ø§Ø²Ø§Øª"],
    "maternity": ["Ø§Ø¬Ø§Ø²Ù‡ Ø§Ù„Ø§Ù…ÙˆÙ…Ù‡","Ø§Ù…ÙˆÙ…Ù‡"],
    "procurement_conflict": ["ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ù…ØµØ§Ù„Ø­","Ø§Ø®Ù„Ø§Ù‚ÙŠØ§Øª Ø§Ù„Ø´Ø±Ø§Ø¡"],
    "per_diem": ["Ù…ÙŠØ§ÙˆÙ…Ø§Øª","Ø¨Ø¯Ù„ Ø³ÙØ±"],
}

def _find_anchor_pages_by_alias_ar(hier, alias_keywords):
    """Robustly iterate headings from various hierarchy shapes (dict/object/list)."""
    pages = []
    if not hier or not alias_keywords:
        return pages

    def _iter_headings(h):
        # dict-like
        if isinstance(h, dict):
            for key in ("headings", "nodes", "sections", "items"):
                v = h.get(key)
                if isinstance(v, list):
                    return v
        # object-like
        for key in ("headings", "nodes", "sections", "items"):
            v = getattr(h, key, None)
            if isinstance(v, list):
                return v
        # maybe nested container
        for key in ("data", "root"):
            v = getattr(h, key, None)
            if isinstance(v, list):
                return v
        # list already
        if isinstance(h, list):
            return h
        return []

    def _hget(obj, key, default=None):
        if isinstance(obj, dict):
            return obj.get(key, default)
        val = getattr(obj, key, None)
        if val is not None:
            return val
        meta = getattr(obj, "meta", None) or getattr(obj, "__dict__", None)
        if isinstance(meta, dict):
            return meta.get(key, default)
        return default

    normalized_aliases = [_normalize_text_ar(a) for a in alias_keywords]
    for h in _iter_headings(hier):
        title = _normalize_text_ar(str(_hget(h, "title", "") or _hget(h, "name", "") or _hget(h, "heading", "")))
        if any(a in title for a in normalized_aliases):
            ps = _hget(h, "page_start", _hget(h, "pageno", _hget(h, "page", None)))
            pe = _hget(h, "page_end", None)
            try:
                if ps is not None:
                    pages.append(int(ps))
                if pe is not None and pe != ps:
                    pages.append(int(pe))
            except Exception:
                pass
    return sorted(set(pages))[:10]

# ---------------- Core Q&A ----------------
def _is_hours_like(question: str, intent: str = "") -> bool:
    q = _normalize_text_ar(question or "")
    base_signals = [
        "Ø³Ø§Ø¹Ø§Øª","Ø¯ÙˆØ§Ù…","Ø§ÙˆÙ‚Ø§Øª","Ø±Ù…Ø¶Ø§Ù†","Ø§Ù„ØµÙˆÙ…","Ø§ÙŠØ§Ù…","Ø§Ø¶Ø§ÙÙŠ","Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø§Ø¶Ø§ÙÙŠÙ‡",
        "Ø§Ø³ØªØ±Ø§Ø­Ù‡","Ù…ØºØ§Ø¯Ø±Ù‡","Ø§Ø°Ù†","ÙˆÙ‚Øª","Ø¨Ø¯Ù„","Ù…ÙŠØ§ÙˆÙ…Ø§Øª","Ø³ÙØ±","Ø±ÙˆØ§ØªØ¨","ØµØ±Ù","Ù…ÙˆØ¹Ø¯",
        "Ø§Ù„Ø¹Ø·Ù„","Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¹Ø·Ù„","ØªØ¹ÙˆÙŠØ¶","Ø§Ø¬Ø±","Ø¬Ø¯Ø§ÙˆÙ„ Ø²Ù…Ù†ÙŠÙ‡","Ø§Ù„Ø­Ø¶ÙˆØ±"
    ]
    has_numeric_hint = bool(re.search(r"\d", _to_western_digits(q))) or ("%" in q or "Ùª" in q)
    if has_numeric_hint:
        return True
    return any(tok in q for tok in base_signals) or intent in (
        "work_hours","ramadan_hours","overtime","work_days","breaks"
    )

def ask_once(index: RET.HybridIndex, tokenizer, model, question: str,
             use_llm: bool = True, use_rerank_flag: bool = True, cfg: SimpleNamespace = None) -> str:
    t0 = time.time()
    cfg = cfg or SimpleNamespace(max_bullets=5, bullet_max_chars=120, paginate_chars=600,
                                 hourlines_only=False, regex_hunt=True)
    intent = RET.classify_intent(question)

    extractive_answer = RET.answer(question, index, intent, use_rerank_flag=use_rerank_flag)

    # split body/sources
    lines = str(extractive_answer or "").split('\n')
    body_lines, source_lines, sources_started = [], [], False
    for line in lines:
        ls = line.strip()
        if ls.startswith("Sources:") or ls.startswith("Ø§Ù„Ù…ØµØ§Ø¯Ø±:"):
            sources_started = True; source_lines.append(line)
        elif sources_started:
            source_lines.append(line)
        else:
            body_lines.append(line)
    body_raw = '\n'.join(body_lines).strip()
    sources = '\n'.join(source_lines).strip()

    # page context from cited pages
    cited_pages = _parse_pages_from_sources(sources)
    page_ctx = _page_ctx_from_pages(cited_pages, max_chars=3500)

    # Arabic-only dynamic anchoring
    alias_list = INTENT_ALIASES_AR.get(intent, [])
    if not alias_list:
        q_norm = _normalize_text_ar(question)
        if any(k in q_norm for k in ["Ø±Ù…Ø¶Ø§Ù†","Ø§Ù„ØµÙˆÙ…"]):
            alias_list = INTENT_ALIASES_AR["ramadan_hours"]
        elif any(k in q_norm for k in ["Ø§Ø¶Ø§ÙÙŠ","Ø§Ù„Ø§Ø¶Ø§ÙÙŠ","Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø§Ø¶Ø§ÙÙŠÙ‡"]):
            alias_list = INTENT_ALIASES_AR["overtime"]
        elif any(k in q_norm for k in ["Ø§Ø¬Ø§Ø²Ù‡ Ø³Ù†ÙˆÙŠÙ‡","Ø§Ù„Ø§Ø¬Ø§Ø²Ù‡ Ø§Ù„Ø³Ù†ÙˆÙŠÙ‡","Ø³Ù†ÙˆÙŠÙ‡"]):
            alias_list = INTENT_ALIASES_AR["annual_leave"]
        elif any(k in q_norm for k in ["Ù…Ø±Ø¶ÙŠÙ‡","Ø§Ù„Ø§Ø¬Ø§Ø²Ù‡ Ø§Ù„Ù…Ø±Ø¶ÙŠÙ‡"]):
            alias_list = INTENT_ALIASES_AR["sick_leave"]
        elif any(k in q_norm for k in ["Ù…ØºØ§Ø¯Ø±Ù‡","Ø§Ø°Ù† Ù…ØºØ§Ø¯Ø±Ù‡"]):
            alias_list = INTENT_ALIASES_AR["hourly_leave"]
        elif any(k in q_norm for k in ["Ø±ÙˆØ§ØªØ¨","ØµØ±Ù Ø§Ù„Ø±ÙˆØ§ØªØ¨","Ø§Ø¬Ø±"]):
            alias_list = INTENT_ALIASES_AR["payroll"]
        elif any(k in q_norm for k in ["Ù…ÙˆØ§ØµÙ„Ø§Øª","Ø¨Ø¯Ù„ Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§Øª"]):
            alias_list = INTENT_ALIASES_AR["transport"]
        elif any(k in q_norm for k in ["Ù…ÙŠØ§ÙˆÙ…Ø§Øª","Ø¨Ø¯Ù„ Ø³ÙØ±"]):
            alias_list = INTENT_ALIASES_AR["per_diem"]
        elif any(k in q_norm for k in ["ØªØ¶Ø§Ø±Ø¨","ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ù…ØµØ§Ù„Ø­"]):
            alias_list = INTENT_ALIASES_AR["procurement_conflict"]
        elif any(k in q_norm for k in ["Ø¹Ø·Ù„","Ø§Ù„Ø¹Ø·Ù„"]):
            alias_list = INTENT_ALIASES_AR["holidays_work"]
        else:
            if _is_hours_like(question, intent):
                alias_list = INTENT_ALIASES_AR["work_hours"]

    try:
        anchor_pages = _find_anchor_pages_by_alias_ar(index.hier, alias_list)
    except Exception as e:
        LOG.warning("anchor_pages failed: %s", e)
        anchor_pages = []
    if anchor_pages:
        anchored_ctx = _page_ctx_from_pages(anchor_pages, max_chars=3500)
        if anchored_ctx:
            page_ctx = anchored_ctx

    # 1) Cleanup + minimal rescue if empty or weak
    hours_like = _is_hours_like(question, intent)
    tmp_body = _clean_text(body_raw)
    if (not tmp_body) or ("Ù„Ø§ ÙŠÙ‚Ø¯Ù‘Ù… Ø§Ù„Ù†Øµ" in tmp_body) or ("Ù„Ù… Ø£Ø¹Ø«Ø±" in tmp_body):
        body_raw = page_ctx if page_ctx else body_raw
    elif (hours_like or _expects_numerics(question)) and not _has_times_or_days(tmp_body):
        if _has_times_or_days(page_ctx):
            body_raw = page_ctx
        else:
            # pick any page with anchor keywords from the question
            q_kws = _question_keywords(question)
            candidate_pages = []
            for p, txt in CHUNKS_BY_PAGE.items():
                t = _clean_text(txt)
                if any(kw in t for kw in q_kws):
                    candidate_pages.append(p)
            if candidate_pages and not page_ctx:
                body_raw = _page_ctx_from_pages(candidate_pages[:6], max_chars=3500)

    # 2) Generic regex hunt (question-driven)
    if cfg.regex_hunt:
        q_kws = _question_keywords(question)
        hunted = []
        # search cited pages first
        for p in cited_pages or []:
            t = CHUNKS_BY_PAGE.get(p, "")
            if not t: continue
            hunted.extend(_regex_hunt_generic(t, q_kws))
            if len(hunted) >= 10: break
        # broaden to anchor pages
        if not hunted:
            anchor_pages2 = []
            for p, txt in CHUNKS_BY_PAGE.items():
                t = _clean_text(txt)
                if any(kw in t for kw in q_kws):
                    anchor_pages2.append(p)
            for p in anchor_pages2:
                t = CHUNKS_BY_PAGE.get(p, "")
                hunted.extend(_regex_hunt_generic(t, q_kws))
                if len(hunted) >= 10: break
        # global fallback
        if not hunted:
            all_text = "\n".join(CHUNKS_BY_PAGE.get(p, "") for p in sorted(CHUNKS_BY_PAGE))
            hunted = _regex_hunt_generic(all_text, q_kws)
        # prefer hunted lines when strict numerics expected
        if hunted and (hours_like or _expects_numerics(question)):
            body_raw = "\n".join(hunted[:8])

    # 3) Final formatting (LLM optional, but we keep it off-safe by default)
    def _final(dt, text):
        # Dedup + shrink sources block
        nonlocal sources
        if sources:
            lines = [l.strip() for l in sources.splitlines() if l.strip()]
            seen = set(); clean = []
            for l in lines:
                k = re.sub(r"(page|ØµÙØ­Ø©)\s+\d+","page", l, flags=re.I)
                if k in seen: continue
                seen.add(k); clean.append(l)
            sources = "\n".join(clean[:5])
        parts = _paginate_text(text, max_chars=cfg.paginate_chars)
        if len(parts) > 1:
            labeled = []
            for i, p in enumerate(parts, 1):
                labeled.append(f"Ø§Ù„Ø¬Ø²Ø¡ {i}/{len(parts)}:\n{p}")
            text = "\n\n".join(labeled)
        return f"â± {dt:.2f}s | ğŸ¤– {text}\n{sources}" if sources else f"â± {dt:.2f}s | ğŸ¤– {text}"

    body_clean = _clean_text(body_raw)
    body_clean = _purge_non_arabic_lines(body_clean)

    # STRICT safety net: if numeric/time expected but none found
    if (not body_clean) or (hours_like or _expects_numerics(question)) and not _has_times_or_days(body_clean):
        if not _has_times_or_days(body_clean):
            dt = time.time() - t0
            msg = "Ù„Ø§ ÙŠØ°ÙƒØ± Ø§Ù„Ù†Øµ Ø±Ù‚Ù…Ø§Ù‹/ÙˆÙ‚ØªØ§Ù‹ Ù…Ø­Ø¯Ø¯Ø§Ù‹ Ù„Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¶Ù…Ù† Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¥Ù„ÙŠÙ‡Ø§."
            return _final(dt, msg)

    # No LLM path (recommended for strict accuracy)
    if (not use_llm) or (tokenizer is None) or (model is None):
        dt = time.time() - t0
        bullets = _bullets_for_display(body_clean or body_raw, question, intent, cfg)
        if not bullets:
            bullets = _as_bullets_clipped(_sentences(body_clean or body_raw), limit=cfg.max_bullets, max_chars=cfg.bullet_max_chars)
        formatted = f"Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ±Ø¬ÙØ¹Ø© Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±ØŒ Ø¥Ù„ÙŠÙƒ Ø§Ù„Ø®Ù„Ø§ØµØ©:\n{bullets}" if bullets else (body_clean or body_raw)
        return _final(dt, formatted)

    # (Optional) LLM refine â€” not necessary; kept for completeness but guarded
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM  # load already done in main
        system_prompt = (
            "Ù„Ø®Ù‘Øµ Ø¨ÙˆØ¶ÙˆØ­ Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¯ÙˆÙ† Ø¥Ø¶Ø§ÙØ© Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø£Ùˆ Ø£Ø±Ù‚Ø§Ù… ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. "
            "Ø£Ø¹Ø¯ Ø¨Ø§Ù„Ø¨Ù†ÙˆØ¯ (â€¢) ÙˆØ¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·."
        )
        user_prompt = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\nØ§Ù„Ù†Øµ:\n{body_clean or body_raw}"
        if hasattr(tokenizer, "apply_chat_template"):
            prompt = tokenizer.apply_chat_template(
                [{"role":"system","content":system_prompt},{"role":"user","content":user_prompt}],
                tokenize=False, add_generation_prompt=True
            )
        else:
            prompt = f"[system]\n{system_prompt}\n\n[user]\n{user_prompt}\n\n[assistant]\n"
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        if hasattr(model, "device"):
            inputs = {k: v.to(model.device) for k,v in inputs.items()}
        eos_id = getattr(tokenizer, "eos_token_id", None)
        pad_id = eos_id if eos_id is not None else getattr(tokenizer, "pad_token_id", None)
        out_ids = model.generate(**inputs, max_new_tokens=160, do_sample=False, repetition_penalty=1.05,
                                 eos_token_id=eos_id, pad_token_id=pad_id)
        start = inputs["input_ids"].shape[1]
        raw = tokenizer.decode(out_ids[0][start:], skip_special_tokens=True).strip()
        resp = _clean_text(raw); resp = _purge_non_arabic_lines(resp)
        if not resp:
            dt = time.time() - t0
            bullets = _bullets_for_display(body_clean or body_raw, question, intent, cfg)
            formatted = f"Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ±Ø¬ÙØ¹Ø© Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±ØŒ Ø¥Ù„ÙŠÙƒ Ø§Ù„Ø®Ù„Ø§ØµØ©:\n{bullets}" if bullets else (body_clean or body_raw)
            return _final(dt, formatted)
        dt = time.time() - t0
        return _final(dt, resp)
    except Exception as e:
        LOG.warning(f"LLM generation failed: {e}")
        dt = time.time() - t0
        bullets = _bullets_for_display(body_clean or body_raw, question, intent, cfg)
        formatted = f"Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø³ØªØ±Ø¬ÙØ¹Ø© Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±ØŒ Ø¥Ù„ÙŠÙƒ Ø§Ù„Ø®Ù„Ø§ØµØ©:\n{bullets}" if bullets else (body_clean or body_raw)
        return _final(dt, formatted)

# ---------------- Sanity runner ----------------
def _gather_sanity_prompts() -> list:
    ret_prompts = []
    try: ret_prompts = list(getattr(RET, "SANITY_PROMPTS", []) or [])
    except Exception: ret_prompts = []
    seen, merged = set(), []
    for q in (ret_prompts + DEFAULT_SANITY_PROMPTS):
        if q not in seen:
            seen.add(q); merged.append(q)
    return merged

def _pass_loose(answer_text: str) -> bool:
    has_sources = ("Sources:" in answer_text) or ("Ø§Ù„Ù…ØµØ§Ø¯Ø±:" in answer_text)
    bad = ("Ù„Ø§ ÙŠÙ‚Ø¯Ù‘Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ±Ø¬ÙØ¹ ØªÙØ§ØµÙŠÙ„ ÙƒØ§ÙÙŠØ©" in answer_text)
    return bool(has_sources and not bad)

def _is_meaningful(txt: str) -> bool:
    return bool(txt and len(re.sub(r"\s+","", txt)) >= 12)

def _pass_strict(question: str, body_only: str) -> bool:
    if not _is_meaningful(body_only): return False
    if _is_hours_like(question, "") or _expects_numerics(question):
        return _has_times_or_days(body_only)
    return True

def run_test_prompts(index: RET.HybridIndex, tokenizer, model,
                     use_llm: bool, use_rerank_flag: bool, artifacts_dir: str, cfg: SimpleNamespace):
    os.makedirs(artifacts_dir, exist_ok=True)
    results_path = os.path.join(artifacts_dir, "results.jsonl")
    summary_md   = os.path.join(artifacts_dir, "summary.md")
    report_txt   = os.path.join(artifacts_dir, "report.txt")

    results_f = open(results_path, "w", encoding="utf-8")
    report_f  = open(report_txt,  "w", encoding="utf-8")

    def _tee(line=""):
        print(line); report_f.write(line + "\n"); report_f.flush()

    tests = _gather_sanity_prompts()
    if not tests:
        _tee("âŒ No sanity prompts available.")
        results_f.close(); report_f.close(); return

    _tee("ğŸ§ª Running sanity prompts ...")
    _tee("=" * 80)

    total = len(tests)
    pass_loose_count, pass_strict_count = 0, 0

    for i, q in enumerate(tests, 1):
        _tee(f"\nğŸ“ Test {i}/{total}: {q}")
        _tee("-" * 60)
        try:
            result = ask_once(index, tokenizer, model, q, use_llm=use_llm, use_rerank_flag=use_rerank_flag, cfg=cfg)
            _tee(result)

            body_only, _src_blk = _split_answer(result)
            loose = _pass_loose(result)
            strict = _pass_strict(q, body_only)

            pass_loose_count += int(loose)
            pass_strict_count += int(strict)

            _tee("âœ… PASS_LOOSE" if loose else "âŒ FAIL_LOOSE")
            _tee("âœ… PASS_STRICT" if strict else "âŒ FAIL_STRICT")
            _tee("=" * 80)

            rec = {
                "index": i, "question": q, "answer": result, "body_only": body_only,
                "pass_loose": loose, "pass_strict": strict,
            }
            results_f.write(json.dumps(rec, ensure_ascii=False) + "\n"); results_f.flush()

        except Exception as e:
            _tee(f"âŒ Error: {e}")
            _tee("=" * 80)

    summary = (
        f"# Sanity Summary\n\n"
        f"- Total: {total}\n"
        f"- PASS_LOOSE: {pass_loose_count}/{total}\n"
        f"- PASS_STRICT: {pass_strict_count}/{total}\n"
        f"\nArtifacts:\n- results.jsonl\n- report.txt\n"
    )
    with open(summary_md, "w", encoding="utf-8") as f:
        f.write(summary)

    _tee(f"\nSummary: PASS_LOOSE {pass_loose_count}/{total} | PASS_STRICT {pass_strict_count}/{total}")
    _tee(f"Artifacts saved in: {artifacts_dir}")

    results_f.close(); report_f.close()

# ---------------- CLI ----------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--chunks", type=str, default="Data_pdf_clean_chunks.jsonl", help="Path to chunks (JSONL/JSON)")
    parser.add_argument("--hier-index", type=str, default="heading_inverted_index.json")
    parser.add_argument("--aliases", type=str, default="section_aliases.json")
    parser.add_argument("--save-index", type=str, default=None)
    parser.add_argument("--load-index", type=str, default=None)
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-7B-Instruct")
    parser.add_argument("--ask", type=str, default=None)
    parser.add_argument("--test", action="store_true")
    parser.add_argument("--sanity", action="store_true")
    parser.add_argument("--no-llm", action="store_true")
    parser.add_argument("--use-4bit", action="store_true")
    parser.add_argument("--use-8bit", action="store_true")
    parser.add_argument("--no-rerank", action="store_true")
    parser.add_argument("--device", type=str, default="auto", choices=["auto","cpu","cuda"])
    parser.add_argument("--out-dir", type=str, default="runs")

    # NEW: general controls
    parser.add_argument("--regex-hunt", action="store_true", help="Generic numeric/time/day hunter (question-driven).")
    parser.add_argument("--hourlines-only", action="store_true", help="Keep only lines with times/days/numbers/% when relevant.")
    parser.add_argument("--max-bullets", type=int, default=5, help="Max bullets.")
    parser.add_argument("--bullet-max-chars", type=int, default=120, help="Max characters per bullet.")
    parser.add_argument("--paginate-chars", type=int, default=600, help="Pagination threshold of body.")

    args = parser.parse_args()
    cfg = SimpleNamespace(
        regex_hunt=args.regex_hunt,
        hourlines_only=args.hourlines_only,
        max_bullets=args.max_bullets,
        bullet_max_chars=args.bullet_max_chars,
        paginate_chars=args.paginate_chars,
    )

    # Artifacts dir
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(args.out_dir, f"run_{timestamp}")
    os.makedirs(run_dir, exist_ok=True)

    # Logger
    global LOG
    LOG = setup_logger(os.path.join(run_dir, "run.log"))
    LOG.info("Artifacts will be saved under: %s", run_dir)

    # Build/load index
    hier = RET.load_hierarchy(args.hier_index, args.aliases)
    if not os.path.exists(args.chunks):
        LOG.error("Chunks file not found: %s", args.chunks); return
    chunks, chunks_hash = RET.load_chunks(path=args.chunks)

    global CHUNKS_BY_PAGE
    CHUNKS_BY_PAGE = _build_page_text_index(chunks)

    index = RET.HybridIndex(chunks, chunks_hash, hier=hier)

    loaded = False
    if args.load_index and os.path.exists(args.load_index):
        try:
            rlog = logging.getLogger("retrival_model"); lvl = rlog.level; rlog.setLevel(logging.ERROR)
            loaded = index.load(args.load_index); rlog.setLevel(lvl)
            if loaded: LOG.info("Index loaded successfully from %s", args.load_index)
        except Exception as e:
            LOG.info("Will rebuild index: %s", e)

    if not loaded:
        LOG.info("Building index ..."); index.build()
        if args.save_index:
            try: index.save(args.save_index); LOG.info("Index saved to %s", args.save_index)
            except Exception as e: LOG.warning("Failed to save index: %s", e)

    # Optional LLM
    tok = mdl = None
    use_llm = not args.no_llm
    if use_llm:
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            use_cuda = (args.device != "cpu") and (torch is not None) and hasattr(torch, "cuda") and torch.cuda.is_available()
            if args.device == "cuda" and not use_cuda:
                LOG.warning("CUDA requested but not available; falling back to CPU.")
            bf16_supported = use_cuda and getattr(torch.cuda, "is_bf16_supported", lambda: False)()
            dtype_fp = torch.bfloat16 if (bf16_supported and torch is not None) else (torch.float16 if use_cuda and torch is not None else None)
            model_kwargs = {"trust_remote_code": True}
            if args.device == "cpu" or not use_cuda:
                model_kwargs["device_map"] = "cpu"
                if torch is not None: model_kwargs["torch_dtype"] = torch.float32
            else:
                model_kwargs["device_map"] = "auto"
                if dtype_fp is not None: model_kwargs["torch_dtype"] = dtype_fp
            if args.use_4bit or args.use_8bit:
                try:
                    model_kwargs["quantization_config"] = BitsAndBytesConfig(
                        load_in_4bit=bool(args.use_4bit),
                        load_in_8bit=bool(args.use_8bit),
                        bnb_4bit_compute_dtype=(torch.bfloat16 if bf16_supported else (torch.float16 if use_cuda else None)),
                    )
                except Exception as e:
                    LOG.warning("Quantization setup failed: %s", e)
            tok = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)
            mdl = AutoModelForCausalLM.from_pretrained(args.model, **model_kwargs)
        except Exception as e:
            LOG.warning("Failed to load LLM (%s); continuing retrieval-only. Error: %s", args.model, e)
            tok = mdl = None; use_llm = False

    use_rerank_flag = not args.no_rerank

    if args.test or args.sanity:
        run_test_prompts(index, tok, mdl, use_llm=use_llm, use_rerank_flag=use_rerank_flag, artifacts_dir=run_dir, cfg=cfg)
        print(f"\nâœ… Saved artifacts under: {run_dir}")
        return

    if args.ask:
        ans = ask_once(index, tok, mdl, args.ask, use_llm=use_llm, use_rerank_flag=use_rerank_flag, cfg=cfg)
        single_path = os.path.join(run_dir, "single_answer.txt")
        with open(single_path, "w", encoding="utf-8") as f: f.write(ans)
        print(ans); print(f"\nâœ… Saved single answer to: {single_path}")
        return

    # Interactive
    print("Ready. Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ (Ø§ÙƒØªØ¨ 'exit' Ù„Ù„Ø®Ø±ÙˆØ¬)\n")
    interactive_path = os.path.join(run_dir, "interactive_transcript.txt")
    with open(interactive_path, "w", encoding="utf-8") as trans:
        while True:
            try: q = input("Ø³Ø¤Ø§Ù„Ùƒ: ").strip()
            except (EOFError, KeyboardInterrupt):
                print("\nExiting."); break
            if not q: continue
            if q.lower() in ("exit","quit","q"): print("Exiting."); break
            ans = ask_once(index, tok, mdl, q, use_llm=use_llm, use_rerank_flag=use_rerank_flag, cfg=cfg)
            print(ans); trans.write(f"\nQ: {q}\n{ans}\n"); trans.flush()
    print(f"\nâœ… Interactive transcript saved to: {interactive_path}")

if __name__ == "__main__":
    main()
