# -*- coding: utf-8 -*-
"""
RAG Orchestrator (Arabic) â€” Fixed version with built-in sanity testing
"""

import os, re, time, argparse, logging
import torch
import hashlib
from typing import List, Tuple

# Your retriever module
import retrival_model as RET

logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
LOG = logging.getLogger("rag_qwen_fixed")

# ---------------- Answer Handler (Minimal interference) ----------------

def ask_once(index: RET.HybridIndex,
             tokenizer, model,
             question: str,
             use_llm: bool = True) -> str:
    t0 = time.time()
    intent = RET.classify_intent(question)
    
    # Use your retrieval system directly - minimal interference
    extractive_answer = RET.answer(question, index, intent, use_rerank_flag=True)
    
    # If not using LLM, return retrieval system's answer directly
    if not use_llm or tokenizer is None or model is None:
        dt = time.time() - t0
        # If retrieval already provided timing, use it; otherwise add timing
        if extractive_answer.startswith("â±"):
            return extractive_answer
        return f"â± {dt:.2f}s | ğŸ¤– {extractive_answer}"
    
    # If retrieval system already gave a good work hours answer, don't interfere
    if intent in ("work_hours", "ramadan_hours") and "Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù…" in extractive_answer and "Ù…Ù†" in extractive_answer and "Ø¥Ù„Ù‰" in extractive_answer:
        dt = time.time() - t0
        if extractive_answer.startswith("â±"):
            return extractive_answer
        return f"â± {dt:.2f}s | ğŸ¤– {extractive_answer}"
    
    # For other cases, try LLM refinement but preserve sources
    # Extract body and sources from retrieval output
    lines = extractive_answer.split('\n')
    body_lines = []
    source_lines = []
    sources_started = False
    
    for line in lines:
        line_stripped = line.strip()
        if line_stripped.startswith("Sources:") or line_stripped.startswith("Ø§Ù„Ù…ØµØ§Ø¯Ø±:"):
            sources_started = True
            source_lines.append(line_stripped)
        elif sources_started and (line_stripped == "" or line_stripped[0].isdigit() or "Data_pdf.pdf" in line_stripped):
            source_lines.append(line_stripped)
        elif not sources_started:
            body_lines.append(line)
    
    body = '\n'.join(body_lines).strip()
    sources = '\n'.join(source_lines).strip()
    
    # If retrieval found nothing useful, don't try LLM
    if not body.strip() or "Ù„Ù… Ø£Ø¹Ø«Ø±" in body or "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª" in body:
        dt = time.time() - t0
        if extractive_answer.startswith("â±"):
            return extractive_answer
        return f"â± {dt:.2f}s | ğŸ¤– {extractive_answer}"
    
    # Use LLM for refinement
    try:
        system_prompt = "Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙˆÙ…Ø®ØªØµØ± Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:"
        user_prompt = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {body}"
        
        msgs = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        prompt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        out_ids = model.generate(
            **inputs,
            max_new_tokens=150,
            temperature=0.1,
            do_sample=False,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
        
        response = tokenizer.decode(out_ids[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()
        response = response.split('\n')[0].strip()
        
        if response and len(response) > 5:
            dt = time.time() - t0
            if sources:
                return f"â± {dt:.2f}s | ğŸ¤– {response}\n{sources}"
            else:
                return f"â± {dt:.2f}s | ğŸ¤– {response}"
            
    except Exception as e:
        LOG.warning(f"LLM generation failed: {e}")
    
    # Fallback to retrieval system's answer
    dt = time.time() - t0
    if extractive_answer.startswith("â±"):
        return extractive_answer
    return f"â± {dt:.2f}s | ğŸ¤– {extractive_answer}"

def run_test_prompts(index: RET.HybridIndex, tokenizer, model, use_llm: bool):
    """Run all 30 sanity prompts as test cases"""
    print("ğŸ§ª Running all 30 test prompts from your sanity suite...")
    print("=" * 80)
    
    # All 30 sanity prompts from your retrieval system
    test_prompts = [
        "Ù…Ø§ Ù‡ÙŠ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù… Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ù…Ù† ÙˆØ¥Ù„Ù‰ØŸ",
        "Ù‡Ù„ ÙŠÙˆØ¬Ø¯ Ù…Ø±ÙˆÙ†Ø© ÙÙŠ Ø§Ù„Ø­Ø¶ÙˆØ± ÙˆØ§Ù„Ø§Ù†ØµØ±Ø§ÙØŸ ÙˆÙƒÙŠÙ ØªÙØ­Ø³Ø¨ Ø¯Ù‚Ø§Ø¦Ù‚ Ø§Ù„ØªØ£Ø®ÙŠØ±ØŸ",
        "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ±Ø§Ø­Ø© Ø®Ù„Ø§Ù„ Ø§Ù„Ø¯ÙˆØ§Ù…ØŸ ÙˆÙƒÙ… Ù…Ø¯ØªÙ‡Ø§ØŸ",
        "Ù…Ø§ Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø´Ù‡Ø± Ø±Ù…Ø¶Ø§Ù†ØŸ ÙˆÙ‡Ù„ ØªØªØºÙŠØ±ØŸ",
        "Ù…Ø§ Ø£ÙŠØ§Ù… Ø§Ù„Ø¯ÙˆØ§Ù… Ø§Ù„Ø±Ø³Ù…ÙŠØŸ ÙˆÙ‡Ù„ Ø§Ù„Ø³Ø¨Øª ÙŠÙˆÙ… Ø¹Ù…Ù„ØŸ",
        "ÙƒÙŠÙ ÙŠÙØ­ØªØ³Ø¨ Ø§Ù„Ø£Ø¬Ø± Ø¹Ù† Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø£ÙŠØ§Ù… Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©ØŸ",
        "Ù…Ø§ Ø§Ù„ØªØ¹ÙˆÙŠØ¶ Ø¹Ù†Ø¯ Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¹Ø·Ù„ Ø§Ù„Ø±Ø³Ù…ÙŠØ©ØŸ",
        "Ù‡Ù„ ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø¥Ø¶Ø§ÙÙŠ Ù„Ù…ÙˆØ§ÙÙ‚Ø© Ù…Ø³Ø¨Ù‚Ø©ØŸ ÙˆÙ…Ù† ÙŠØ¹ØªÙ…Ø¯Ù‡Ø§ØŸ",
        "ÙƒÙ… Ù…Ø¯Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø³Ù†ÙˆÙŠØ© Ù„Ù…ÙˆØ¸Ù Ø¬Ø¯ÙŠØ¯ØŸ ÙˆÙ…ØªÙ‰ ØªØ²ÙŠØ¯ØŸ",
        "Ù‡Ù„ ØªÙØ±Ø­Ù‘Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø²Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©ØŸ ÙˆÙ…Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ØŸ",
        "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø·Ø§Ø±Ø¦Ø©ØŸ ÙˆÙƒÙŠÙ Ø£Ø·Ù„Ø¨Ù‡Ø§ØŸ",
        "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ù…Ø±Ø¶ÙŠØ©ØŸ ÙˆØ¹Ø¯Ø¯ Ø£ÙŠØ§Ù…Ù‡Ø§ØŸ ÙˆÙ‡Ù„ ÙŠÙ„Ø²Ù… ØªÙ‚Ø±ÙŠØ± Ø·Ø¨ÙŠØŸ",
        "ÙƒÙ… Ù…Ø¯Ø© Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø£Ù…ÙˆÙ…Ø©ØŸ ÙˆÙ‡Ù„ ÙŠÙ…ÙƒÙ† Ø£Ø®Ø° Ø¬Ø²Ø¡ Ù‚Ø¨Ù„ Ø§Ù„ÙˆÙ„Ø§Ø¯Ø©ØŸ",
        "Ù…Ø§ Ù‡ÙŠ Ø¥Ø¬Ø§Ø²Ø© Ø§Ù„Ø­Ø¯Ø§Ø¯ØŸ Ù„Ù…Ù† ØªÙÙ…Ù†Ø­ ÙˆÙƒÙ… Ù…Ø¯ØªÙ‡Ø§ØŸ",
        "Ù…ØªÙ‰ ÙŠØªÙ… ØµØ±Ù Ø§Ù„Ø±ÙˆØ§ØªØ¨ Ø´Ù‡Ø±ÙŠÙ‹Ø§ØŸ",
        "Ù…Ø§ Ù‡Ùˆ Ø¨Ø¯Ù„ Ø§Ù„Ù…ÙˆØ§ØµÙ„Ø§ØªØŸ ÙˆÙ‡Ù„ ÙŠØ´Ù…Ù„ Ø§Ù„Ø°Ù‡Ø§Ø¨ Ù…Ù† Ø§Ù„Ù…Ù†Ø²Ù„ Ù„Ù„Ø¹Ù…Ù„ØŸ ÙˆÙƒÙŠÙ ÙŠÙØµØ±ÙØŸ",
        "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø³Ù„Ù Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§ØªØ¨ØŸ ÙˆÙ…Ø§ Ø´Ø±ÙˆØ·Ù‡Ø§ØŸ",
        "Ù…Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù†Ø«Ø±ÙŠØ§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©ØŸ ÙˆÙƒÙŠÙ ØªØªÙ… Ø§Ù„ØªØ³ÙˆÙŠØ© ÙˆØ§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŸ",
        "Ù…Ø§ Ø³Ù‚Ù Ø§Ù„Ø´Ø±Ø§Ø¡ Ø§Ù„Ø°ÙŠ ÙŠØ³ØªÙ„Ø²Ù… Ø«Ù„Ø§Ø«Ø© Ø¹Ø±ÙˆØ¶ Ø£Ø³Ø¹Ø§Ø±ØŸ",
        "Ù…Ø§ Ø¶ÙˆØ§Ø¨Ø· ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ù…ØµØ§Ù„Ø­ ÙÙŠ Ø§Ù„Ù…Ø´ØªØ±ÙŠØ§ØªØŸ",
        "Ù…Ø§ Ø­Ø¯ÙˆØ¯ Ù‚Ø¨ÙˆÙ„ Ø§Ù„Ù‡Ø¯Ø§ÙŠØ§ ÙˆØ§Ù„Ø¶ÙŠØ§ÙØ©ØŸ ÙˆÙ…ØªÙ‰ ÙŠØ¬Ø¨ Ø§Ù„Ø¥Ø¨Ù„Ø§ØºØŸ",
        "ÙƒÙŠÙ Ø£Ø³ØªÙ„Ù… Ø¹Ù‡Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø©ØŸ ÙˆÙ…Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ØŸ",
        "ÙƒÙŠÙ Ø£Ø³Ù„Ù‘Ù… Ø§Ù„Ø¹Ù‡Ø¯Ø© Ø¹Ù†Ø¯ Ø§Ù„Ø§Ø³ØªÙ‚Ø§Ù„Ø© Ø£Ùˆ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ØŸ",
        "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨ÙØ¹Ø¯/Ù…Ù† Ø§Ù„Ù…Ù†Ø²Ù„ØŸ ÙˆÙƒÙŠÙ ÙŠØªÙ… Ø§Ø¹ØªÙ…Ø§Ø¯Ù‡ØŸ",
        "ÙƒÙŠÙ Ø£Ù‚Ø¯Ù‘Ù… Ø¥Ø°Ù† Ù…ØºØ§Ø¯Ø±Ø© Ø³Ø§Ø¹ÙŠØ©ØŸ ÙˆÙ…Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ø§Ù„Ø´Ù‡Ø±ÙŠØŸ",
        "Ù…ØªÙ‰ ÙŠØªÙ… ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø³Ù†ÙˆÙŠØŸ ÙˆÙ…Ø§ Ù…Ø¹Ø§ÙŠÙŠØ±Ù‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŸ",
        "Ù…Ø§ Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¥Ù†Ø°Ø§Ø± ÙˆØ§Ù„ØªØ¯Ø±Ù‘Ø¬ Ø§Ù„ØªØ£Ø¯ÙŠØ¨ÙŠ Ù„Ù„Ù…Ø®Ø§Ù„ÙØ§ØªØŸ",
        "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø³Ø±ÙŠØ© ÙˆØ­Ù…Ø§ÙŠØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŸ",
        "Ù…Ø§ Ø³ÙŠØ§Ø³Ø© Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…Ù‡Ù†ÙŠ ÙˆÙ…ÙƒØ§ÙØ­Ø© Ø§Ù„ØªØ­Ø±Ø´ØŸ",
        "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ù…ÙŠØ§ÙˆÙ…Ø§Øª/Ø¨Ø¯Ù„ Ø³ÙØ±ØŸ ÙˆÙƒÙŠÙ ØªÙØµØ±Ù"
    ]
    
    for i, q in enumerate(test_prompts, 1):
        print(f"\nğŸ“ Test {i}: {q}")
        print("-" * 60)
        try:
            result = ask_once(index, tokenizer, model, q, use_llm=use_llm)
            print(result)
        except Exception as e:
            print(f"âŒ Error: {e}")
        print("=" * 80)

# ---------------- CLI ----------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--chunks", type=str, default="Data_pdf_clean_chunks.jsonl")
    ap.add_argument("--hier-index", type=str, default="heading_inverted_index.json")
    ap.add_argument("--aliases", type=str, default="section_aliases.json")
    ap.add_argument("--save-index", type=str, default=None)
    ap.add_argument("--load-index", type=str, default=None)
    ap.add_argument("--model", type=str, default="Qwen/Qwen2.5-7B-Instruct")
    ap.add_argument("--ask", type=str, default=None)
    ap.add_argument("--test", action="store_true", help="Run all 30 built-in test prompts")
    ap.add_argument("--sanity", action="store_true", help="Run all 30 built-in test prompts (alias for --test)")
    ap.add_argument("--no-llm", action="store_true")
    ap.add_argument("--use-4bit", action="store_true")
    ap.add_argument("--use-8bit", action="store_true")
    args = ap.parse_args()

    # Build/load index from your retriever
    hier = RET.load_hierarchy(args.hier_index, args.aliases)
    
    # Load chunks and calculate hash
    if not os.path.exists(args.chunks):
        LOG.error("Chunks file not found: %s", args.chunks)
        return
    
    chunks, chunks_hash = RET.load_chunks(path=args.chunks)
    index = RET.HybridIndex(chunks, chunks_hash, hier=hier)

    # Try to load existing index with better error handling
    loaded = False
    if args.load_index and os.path.exists(args.load_index):
        try:
            # Temporarily suppress warnings
            import logging
            retrieval_logger = logging.getLogger("retrival_model")
            original_level = retrieval_logger.level
            retrieval_logger.setLevel(logging.ERROR)
            
            loaded = index.load(args.load_index)
            
            # Restore original logging level
            retrieval_logger.setLevel(original_level)
            
            if loaded:
                LOG.info("Index loaded successfully")
        except Exception as e:
            LOG.info(f"Will rebuild index: {e}")
    
    # Build index if not loaded
    if not loaded:
        LOG.info("Building index...")
        index.build()
        if args.save_index:
            try:
                index.save(args.save_index)
                LOG.info("Index saved successfully")
            except Exception as e:
                LOG.warning(f"Failed to save index: {e}")

    # Optional LLM
    tok = mdl = None
    use_llm = not args.no_llm
    if use_llm:
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
        
        model_kwargs = {
            "trust_remote_code": True,
            "torch_dtype": torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        }
        
        if torch.cuda.is_available():
            model_kwargs["device_map"] = "auto"
        else:
            model_kwargs["device_map"] = "cpu"
            model_kwargs["torch_dtype"] = torch.float32

        if args.use_4bit or args.use_8bit:
            try:
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True if args.use_4bit else False,
                    load_in_8bit=True if args.use_8bit else False,
                    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
                )
                model_kwargs["quantization_config"] = quant_config
            except Exception as e:
                LOG.warning(f"Quantization failed: {e}")

        tok = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)
        mdl = AutoModelForCausalLM.from_pretrained(args.model, **model_kwargs)

    # Test mode - run all 30 sanity prompts (support both --test and --sanity)
    if args.test or args.sanity:
        run_test_prompts(index, tok, mdl, use_llm=use_llm)
        return

    # Single question mode
    if args.ask:
        print(ask_once(index, tok, mdl, args.ask, use_llm=use_llm))
        return

    # Interactive mode
    print("Ready. Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ (Ø§ÙƒØªØ¨ 'exit' Ù„Ù„Ø®Ø±ÙˆØ¬)\n")
    while True:
        try:
            q = input("Ø³Ø¤Ø§Ù„Ùƒ: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nExiting."); break
        if not q:
            continue
        if q.lower() in ("exit","quit","q"):
            print("Exiting."); break
        print(ask_once(index, tok, mdl, q, use_llm=use_llm))

if __name__ == "__main__":
    main()
