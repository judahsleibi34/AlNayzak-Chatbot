# ==================== PATCH FOR BITSANDBYTES METADATA ISSUE ====================
import importlib.metadata

_original_version = importlib.metadata.version

def patched_version(package_name: str) -> str:
    if package_name == "bitsandbytes":
        return "0.47.0"  # or whatever version you installed
    return _original_version(package_name)

importlib.metadata.version = patched_version
# ==============================================================================

import json
import numpy as np
from typing import List, Dict, Tuple, Optional
import re
from dataclasses import dataclass
from sentence_transformers import SentenceTransformer
import faiss
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig
)
import warnings
import os
from pathlib import Path
import pickle
import time
from collections import defaultdict

warnings.filterwarnings("ignore")

@dataclass
class DocumentChunk:
    id: str
    text_display: str
    text_embed: str
    language: str
    source: str
    page: int
    chunk_no: int
    embedding: Optional[np.ndarray] = None

class OptimizedArabicSemanticRetriever:
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        print("ğŸ”„ Loading embedding model...")
        start_time = time.time()
        
        self.model = SentenceTransformer(model_name)
        self.model.max_seq_length = 256
        
        print(f"âœ… Model loaded in {time.time() - start_time:.2f} seconds")
        
        self.chunks: List[DocumentChunk] = []
        self.embeddings: Optional[np.ndarray] = None
        self.faiss_index: Optional[faiss.Index] = None
        self.cache_dir = Path("./cache")
        self.cache_dir.mkdir(exist_ok=True)
        
    def preprocess_arabic_text(self, text: str) -> str:
        if not text:
            return ""
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[\u064B-\u065F\u0670\u0640]', '', text)
        return text.strip()
    
    def sliding_window_chunk(self, text: str, window_size: int = 700, overlap: int = 180) -> List[str]:
        if len(text) <= window_size:
            return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = min(start + window_size, len(text))
            chunk = text[start:end]
            chunks.append(chunk)
            start += (window_size - overlap)
        return chunks

    def load_documents(self, json_data: List[Dict]) -> None:  # âœ… FIXED SIGNATURE
        print(f"ğŸ“š Loading {len(json_data)} documents with sliding window chunking...")
        self.chunks = []
        
        for i, item in enumerate(json_data):
            if i % 1000 == 0:
                print(f"  Processing document {i}/{len(json_data)}")
                
            full_text = item.get('text_display', '')
            if not full_text.strip():
                continue
                
            chunks = self.sliding_window_chunk(full_text, window_size=700, overlap=180)
            
            for j, chunk_text in enumerate(chunks):
                embed_text = self.preprocess_arabic_text(chunk_text)
                if embed_text.strip():
                    chunk = DocumentChunk(
                        id=f"{item['id']}_chunk{j}",
                        text_display=chunk_text,
                        text_embed=embed_text,
                        language=item.get('language', 'ar'),
                        source=item.get('source', 'unknown'),
                        page=item.get('page', 0),
                        chunk_no=j
                    )
                    self.chunks.append(chunk)
        
        print(f"âœ… Loaded {len(self.chunks)} valid chunks (after sliding window)")
    
    def _get_cache_path(self, data_hash: str) -> Path:
        return self.cache_dir / f"embeddings_{data_hash}.pkl"
    
    def _get_data_hash(self) -> str:
        texts = [chunk.text_embed for chunk in self.chunks[:100]]
        data_str = ''.join(texts)
        return str(hash(data_str))
    
    def build_embeddings(self, use_cache: bool = True, batch_size: int = 128) -> None:
        if not self.chunks:
            raise ValueError("No documents loaded.")
        
        data_hash = self._get_data_hash()
        cache_path = self._get_cache_path(data_hash)
        
        if use_cache and cache_path.exists():
            print("ğŸ“¦ Loading embeddings from cache...")
            try:
                with open(cache_path, 'rb') as f:
                    self.embeddings = pickle.load(f)
                
                for i, chunk in enumerate(self.chunks):
                    if i < len(self.embeddings):
                        chunk.embedding = self.embeddings[i]
                
                self.build_faiss_index()
                print(f"âœ… Loaded cached embeddings for {len(self.chunks)} chunks")
                return
            except Exception as e:
                print(f"âš ï¸ Cache loading failed: {e}, rebuilding...")
        
        print("ğŸ”„ Building embeddings...")
        start_time = time.time()
        
        texts = [chunk.text_embed for chunk in self.chunks]
        all_embeddings = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            batch_embeddings = self.model.encode(
                batch_texts, 
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            all_embeddings.append(batch_embeddings)
            current_batch = (i // batch_size) + 1
            print(f"  Batch {current_batch}/{total_batches} processed")
        
        self.embeddings = np.vstack(all_embeddings)
        
        for i, chunk in enumerate(self.chunks):
            chunk.embedding = self.embeddings[i]
        
        if use_cache:
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(self.embeddings, f)
                print("ğŸ’¾ Embeddings cached for future use")
            except Exception as e:
                print(f"âš ï¸ Failed to cache embeddings: {e}")
        
        self.build_faiss_index()
        elapsed = time.time() - start_time
        print(f"âœ… Built embeddings for {len(self.chunks)} chunks in {elapsed:.2f} seconds")
    
    def build_faiss_index(self) -> None:
        dimension = self.embeddings.shape[1]
        
        if len(self.embeddings) < 10000:
            self.faiss_index = faiss.IndexFlatIP(dimension)
            self.faiss_index.add(self.embeddings.astype('float32'))
        else:
            nlist = min(100, len(self.embeddings) // 100)
            quantizer = faiss.IndexFlatIP(dimension)
            self.faiss_index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
            self.faiss_index.train(self.embeddings.astype('float32'))
            self.faiss_index.add(self.embeddings.astype('float32'))
    
    def semantic_search(self, query: str, top_k: int = 20) -> List[Tuple[DocumentChunk, float]]:
        processed_query = self.preprocess_arabic_text(query)
        query_embedding = self.model.encode([processed_query], normalize_embeddings=True)
        
        scores, indices = self.faiss_index.search(query_embedding.astype('float32'), top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx != -1 and idx < len(self.chunks):
                results.append((self.chunks[idx], float(score)))
        
        return results

    def rerank_with_cross_encoder(self, query: str, candidates: List[Tuple[DocumentChunk, float]], top_k_final: int = 5) -> List[Tuple[DocumentChunk, float]]:
        print("ğŸ”„ Reranking top 20 â†’ 5 with cross-encoder (stub)...")
        return sorted(candidates, key=lambda x: x[1], reverse=True)[:top_k_final]
    
    def get_relevant_context(self, query: str, max_context_length: int = 1500) -> Tuple[str, List[Dict], float]:
        faiss_results = self.semantic_search(query, top_k=20)
        reranked_results = self.rerank_with_cross_encoder(query, faiss_results, top_k_final=5)
        
        context_parts = []
        metadata = []
        total_length = 0
        total_score = 0.0
        used_chunks = 0
        
        for i, (chunk, score) in enumerate(reranked_results):
            chunk_text = chunk.text_display.strip()
            source_info = f"[Ø§Ù„Ù…ØµØ¯Ø±: {chunk.source} - Øµ{chunk.page}]"
            
            proposed_length = total_length + len(chunk_text) + len(source_info) + 2
            
            if proposed_length <= max_context_length:
                context_parts.append(f"{source_info}\n{chunk_text}")
                metadata.append({
                    "source": chunk.source,
                    "page": chunk.page,
                    "score": score,
                    "chunk_id": chunk.id
                })
                total_length = proposed_length
                total_score += score
                used_chunks += 1
            else:
                remaining = max_context_length - total_length - len(source_info) - 2
                if remaining > 200:
                    truncated_text = chunk_text[:remaining-50] + " ... [Ù…Ù‚Ø·Ø¹ Ù…Ø®ØªØµØ±]"
                    context_parts.append(f"{source_info}\n{truncated_text}")
                    metadata.append({
                        "source": chunk.source,
                        "page": chunk.page,
                        "score": score,
                        "chunk_id": chunk.id,
                        "truncated": True
                    })
                    total_length += len(truncated_text) + len(source_info) + 2
                    total_score += score * 0.9
                    used_chunks += 1
                break
        
        avg_score = total_score / used_chunks if used_chunks > 0 else 0.0
        context = "\n\n".join(context_parts)
        
        return context, metadata, avg_score

class OptimizedSambaLingoRAG:
    def __init__(self, jsonl_filepath: str, use_cache: bool = True):
        print("ğŸš€ Initializing Enhanced Arabic RAG System...")
        start_time = time.time()
        
        print("ğŸ“š Setting up semantic retriever...")
        self.retriever = self._setup_retriever(jsonl_filepath, use_cache)
        
        print("ğŸ§  Loading SambaLingo Arabic model...")
        self.tokenizer, self.model = self._setup_sambanova_model()
        
        total_time = time.time() - start_time
        print(f"âœ… RAG System initialized in {total_time:.2f} seconds!")
    
    def _load_jsonl_fast(self, filepath: str) -> List[Dict]:
        data = []
        try:
            with open(filepath, 'r', encoding='utf-8') as file:
                lines = file.readlines()
                
            print(f"ğŸ“– Processing {len(lines)} lines...")
            
            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                if line:
                    try:
                        data.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        if line_num <= 10:
                            print(f"âš ï¸ Error parsing line {line_num}: {e}")
                
                if line_num % 5000 == 0:
                    print(f"  Processed {line_num}/{len(lines)} lines")
            
            print(f"âœ… Loaded {len(data)} valid chunks from {filepath}")
            return data
        except FileNotFoundError:
            print(f"âŒ File {filepath} not found!")
            return []
        except Exception as e:
            print(f"âŒ Error loading file: {e}")
            return []
    
    def _setup_retriever(self, jsonl_filepath: str, use_cache: bool) -> OptimizedArabicSemanticRetriever:
        data = self._load_jsonl_fast(jsonl_filepath)
        if not data:  # âœ… FIXED CONDITION
            raise ValueError("No data loaded from JSONL file")
        
        retriever = OptimizedArabicSemanticRetriever()
        retriever.load_documents(data)
        retriever.build_embeddings(use_cache=use_cache, batch_size=128)
        return retriever
    
    def _setup_sambanova_model(self):
        model_name = "sambanovasystems/SambaLingo-Arabic-Chat"
        
        # âœ… SAFETY CHECK FOR GPU
        if not torch.cuda.is_available():
            print("âš ï¸ No GPU detected. Loading model on CPU may be very slow.")
        
        # âœ… TRY BNB CONFIG WITH FALLBACK
        try:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                llm_int8_enable_fp32_cpu_offload=True
            )
            use_bnb = True
        except Exception:
            print("âš ï¸ bitsandbytes not available, falling back to non-quantized load.")
            bnb_config, use_bnb = None, False
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side="left"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model_kwargs = {
            "device_map": "auto",
            "trust_remote_code": True,
            "torch_dtype": torch.float16,
            "low_cpu_mem_usage": True,
            "use_cache": True,
        }
        
        if use_bnb:
            model_kwargs["quantization_config"] = bnb_config
        else:
            model_kwargs["torch_dtype"] = torch.float16 if torch.cuda.is_available() else torch.bfloat16
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            **model_kwargs
        )
        
        model.eval()
        if hasattr(model, 'gradient_checkpointing_enable'):
            model.gradient_checkpointing_enable()
        
        print(f"ğŸ“Š Model loaded on: {model.device}")
        memory_gb = model.get_memory_footprint() / 1024**3 if hasattr(model, 'get_memory_footprint') else 0
        print(f"ğŸ’¾ Model memory footprint: ~{memory_gb:.1f}GB")
        
        return tokenizer, model
    
    def _create_structured_prompt(self, question: str, context: str) -> str:
        """SIMPLE, CLEAR PROMPT THAT WORKS"""
        prompt = f"""Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªÙˆÙØ±:

Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

Ø£Ø¬Ø¨ Ø¨ØªÙ†Ø³ÙŠÙ‚ Ø¹Ø±Ø¨ÙŠ Ù…Ù†Ø¸Ù… ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:
- Ø§Ù„Ù†Ø·Ø§Ù‚:
- Ø§Ù„Ø£Ù‡Ù„ÙŠØ©:
- Ø§Ù„Ø®Ø·ÙˆØ§Øª:
- Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª:
- Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª:
- Ù…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ:
- Ø«ØºØ±Ø§Øª: (Ø¥Ù† ÙˆØ¬Ø¯Øª)

Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø©. Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ¯Ø± [ØµX] Ø¹Ù†Ø¯ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ù‚Ø³Ù….
"""
        return prompt

    def _compute_confidence(self, similarity_score: float, context_metadata: List[Dict], generated_answer: str) -> Dict:  # âœ… FIXED SIGNATURE
        signals = {}
        signals['similarity'] = min(max(similarity_score, 0.0), 1.0)
        
        required_sections = ["Ø§Ù„Ù†Ø·Ø§Ù‚", "Ø§Ù„Ø£Ù‡Ù„ÙŠØ©", "Ø§Ù„Ø®Ø·ÙˆØ§Øª", "Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª", "Ø§Ù„Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª", "Ù…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ"]
        found_sections = sum(1 for s in required_sections if s in generated_answer)  # âœ… CLEANER
        signals['coverage'] = found_sections / len(required_sections) if required_sections else 1.0
        
        mentioned_pages = re.findall(r'\[Øµ(\d+)\]', generated_answer)
        valid_pages = {str(meta['page']) for meta in context_metadata}
        if mentioned_pages:
            valid_mentions = sum(1 for p in mentioned_pages if p in valid_pages)
            signals['faithfulness'] = valid_mentions / len(mentioned_pages)
        else:
            signals['faithfulness'] = 1.0
        
        weights = {'similarity': 0.4, 'coverage': 0.3, 'faithfulness': 0.3}
        confidence = sum(weights[k] * signals[k] for k in weights)  # âœ… CLEANER
        
        return {
            "confidence": confidence,
            "signals": signals,
            "coverage_score": signals['coverage']
        }
    
    def _post_process_answer(self, answer: str, prompt: str) -> str:
        """Remove prompt echo and clean up"""
        # Remove the prompt from the beginning
        if answer.startswith(prompt):
            answer = answer[len(prompt):].strip()
        
        # Remove any remaining instruction echoes
        answer = re.sub(r'^.*?Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ.*?\n', '', answer, flags=re.DOTALL)
        answer = re.sub(r'^.*?Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰.*?:', '', answer, flags=re.DOTALL)
        
        # Clean artifacts
        answer = answer.replace("<|end_header_id|>", "").strip()
        
        # Ensure structure
        if "Ù…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ" not in answer:
            answer += "\n\nÙ…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©."
        
        return answer.strip()
    
    def generate_answer(self, question: str, max_length: int = 500, temperature: float = 0.3, 
                       confidence_threshold: float = 0.5) -> Dict:
        start_time = time.time()
        
        print("ğŸ” Retrieving context...")
        context, metadata, avg_similarity = self.retriever.get_relevant_context(question, max_context_length=1500)
        
        if not context.strip():
            return {
                "answer": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªÙˆÙØ± Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.",
                "context": "",
                "metadata": [],
                "confidence": 0.0,
                "signals": {},
                "coverage_score": 0.0,
                "time_taken": time.time() - start_time,
                "was_truncated": False
            }
        
        prompt = self._create_structured_prompt(question, context)
        
        print("ğŸ§  Generating answer...")
        try:
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                truncation=True, 
                max_length=1500,
                padding=False
            ).to(self.model.device)
            
            generate_kwargs = {
                "max_new_tokens": max_length,
                "do_sample": True,
                "temperature": temperature,
                "top_p": 0.92,
                "repetition_penalty": 1.15,
                "pad_token_id": self.tokenizer.eos_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
                "use_cache": True,
            }

            with torch.no_grad():
                outputs = self.model.generate(**inputs, **generate_kwargs)
            
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract and clean answer
            answer = self._post_process_answer(full_response, prompt)
            
            # Compute confidence
            confidence_data = self._compute_confidence(avg_similarity, metadata, answer)
            
            # Confidence-based fallback
            if confidence_data["confidence"] < confidence_threshold or confidence_data["coverage_score"] < 0.6:
                fallback = (
                    "Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§ Ø¨Ø³Ø¨Ø¨ Ù…Ø­Ø¯ÙˆØ¯ÙŠØ© Ø§Ù„Ø³ÙŠØ§Ù‚.\n\n"
                    "Ù…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ: ØºÙŠØ± Ù…ØªÙˆÙØ± Ø¨Ø³Ø¨Ø¨ Ù†Ù‚Øµ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª.\n"
                    "Ø«ØºØ±Ø§Øª: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© Ø­ÙˆÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹."
                )
                answer = fallback
                confidence_data["confidence"] = 0.3
            
            time_taken = time.time() - start_time
            print(f"âš¡ Answer generated in {time_taken:.2f} seconds | Confidence: {confidence_data['confidence']:.2f}")
            
            return {
                "answer": answer,
                "context": context,
                "metadata": metadata,
                "confidence": confidence_data["confidence"],
                "signals": confidence_data["signals"],
                "coverage_score": confidence_data["coverage_score"],
                "time_taken": time_taken,
                "was_truncated": any(m.get('truncated', False) for m in metadata)
            }
            
        except Exception as e:
            print(f"âŒ Error generating answer: {e}")
            return {
                "answer": "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
                "context": context,
                "metadata": metadata,
                "confidence": 0.0,
                "signals": {},
                "coverage_score": 0.0,
                "time_taken": time.time() - start_time,
                "was_truncated": any(m.get('truncated', False) for m in metadata)
            }
    
    def ask(self, question: str) -> str:
        result = self.generate_answer(question)
        return result["answer"]
    
    def benchmark(self, questions: List[str]) -> None:
        print(f"\nğŸ§ª Running benchmark with your questions...")
        print("="*80)
        
        total_time = 0
        total_confidence = 0
        total_coverage = 0
        
        for i, question in enumerate(questions, 1):
            print(f"\n{i}. Ø³Ø¤Ø§Ù„: {question}")
            result = self.generate_answer(question)
            print(f"â±ï¸ Ø§Ù„ÙˆÙ‚Øª: {result['time_taken']:.2f}s | ğŸ¯ Ø§Ù„Ø«Ù‚Ø©: {result['confidence']:.2f} | ğŸ“Š Ø§Ù„ØªØºØ·ÙŠØ©: {result['coverage_score']:.2f}")
            
            # Show first 400 chars of answer
            answer_preview = result['answer'][:400]
            if len(result['answer']) > 400:
                answer_preview += "..."
            print(f"ğŸ“„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {answer_preview}")
            
            if result['was_truncated']:
                print("âš ï¸  Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø§Ù‚ØªØ·Ø§Ø¹ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚")
            total_time += result['time_taken']
            total_confidence += result['confidence']
            total_coverage += result['coverage_score']
            print("-"*80)
        
        n = len(questions)
        avg_time = total_time / n
        avg_conf = total_confidence / n
        avg_cov = total_coverage / n
        
        print(f"\nğŸ“Š Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆÙ‚Øª: {avg_time:.2f}s | Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {avg_conf:.2f} | Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØºØ·ÙŠØ©: {avg_cov:.2f}")
        print(f"ğŸ“Š Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_time:.2f}s")
    
    def chat_loop(self):
        print("\n" + "="*80)
        print("ğŸ¤– Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø°ÙƒÙŠ")
        print("   - 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø§Ù†ØªÙ‡Ø§Ø¡")
        print("="*80 + "\n")
        
        while True:
            try:
                question = input("ğŸ™‹ Ø³Ø¤Ø§Ù„Ùƒ: ").strip()
                
                if question.lower() in ['Ø®Ø±ÙˆØ¬', 'exit', 'quit']:
                    print("ğŸ™ Ø´ÙƒØ±Ø§Ù‹!")
                    break
                
                if not question:
                    continue
                
                print()
                result = self.generate_answer(question)
                print(f"ğŸ¤– {result['answer']}")
                print(f"\nğŸ“ˆ Ø§Ù„Ø«Ù‚Ø©: {result['confidence']:.2f}")
                print()
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ØªÙ… Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù")
                break
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£: {e}")
    
    def cleanup(self):
        print("ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©...")
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'tokenizer'):
            del self.tokenizer
        if hasattr(self, 'retriever'):
            del self.retriever
        
        torch.cuda.empty_cache()
        print("âœ… ØªÙ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©")

# Main execution with your questions
if __name__ == "__main__":
    try:
        rag_system = OptimizedSambaLingoRAG("arabic_chatbot_knowledge.jsonl", use_cache=True)
        
        # Your test questions
        test_questions = [
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¸Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯ ØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§ Ø¹Ù†Ø¯ Ø§Ù„ØªØ¹ÙŠÙŠÙ† ÙÙŠ Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ù†ÙŠØ²ÙƒØŸ",
            "Ù…Ø§ Ù‡ÙŠ Ø´Ø±ÙˆØ· Ø§Ù„Ø£Ù‡Ù„ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¹Ù„Ù‰ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ÙÙŠ Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ù†ÙŠØ²ÙƒØŸ",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¦ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ…Ø± Ø¨Ù‡Ø§ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙˆØ¸ÙŠÙ Ø¯Ø§Ø®Ù„ Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ù†ÙŠØ²Ùƒ Ø¨Ø¯Ø¡Ù‹Ø§ Ù…Ù† Ø§Ù„Ø¥Ø¹Ù„Ø§Ù† ÙˆØ­ØªÙ‰ ØªÙˆÙ‚ÙŠØ¹ Ø§Ù„Ø¹Ù‚Ø¯ØŸ",
            "Ù…Ø§ Ù‡Ùˆ Ù†Ø·Ø§Ù‚ Ø³ÙŠØ§Ø³Ø© Ø§Ù„ØªÙˆØ¸ÙŠÙ ÙÙŠ Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ù†ÙŠØ²ÙƒØŸ ÙˆÙ‡Ù„ ØªØ´Ù…Ù„ Ø§Ù„Ù…ÙˆØ¸ÙÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚ØªÙŠÙ† ÙˆØ§Ù„Ù…Ø³ØªØ´Ø§Ø±ÙŠÙ† Ø£Ù… ÙÙ‚Ø· Ø§Ù„Ø¹Ø§Ù…Ù„ÙŠÙ† Ø§Ù„Ø¯Ø§Ø¦Ù…ÙŠÙ†ØŸ",
            "Ù‡Ù„ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª ÙÙŠ Ø³ÙŠØ§Ø³Ø© Ø§Ù„ØªÙˆØ¸ÙŠÙ Ø¨Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ù†ÙŠØ²Ùƒ Ù…Ø«Ù„ ØªØ¹ÙŠÙŠÙ† Ø®Ø¨Ø±Ø§Ø¡ Ø¨Ø¹Ù‚ÙˆØ¯ Ø®Ø§ØµØ© Ø£Ùˆ Ù„Ù…Ù‡Ø§Ù… Ù…Ø­Ø¯Ø¯Ø©ØŸ"
        ]
        
        print("ğŸ§ª Running benchmark with your detailed questions...")
        rag_system.benchmark(test_questions)
        
        print("\nğŸ’¬ Starting chat...")
        rag_system.chat_loop()
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø¸Ø§Ù…")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if 'rag_system' in locals():
            rag_system.cleanup()
