# -*- coding: utf-8 -*-
"""
RAG Orchestrator (Arabic) ‚Äî runs sanity prompts easily.
- Adds --sanity flag (alias of --test).
- If RET.SANITY_PROMPTS is missing, falls back to DEFAULT_SANITY_PROMPTS (below).
- Retrieval-first; optional LLM refinement via Transformers (can be disabled with --no-llm).
"""

import os
import time
import argparse
import logging

# ---- Optional: reduce TF/XLA noise (safe if TF not installed) ----
os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")  # 0=all,1=info,2=warning,3=error

# Optional torch (for dtype/device checks)
try:
    import torch
except Exception:
    torch = None

# Your retriever module (as in your original code)
import retrival_model as RET

logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
LOG = logging.getLogger("rag_orchestrator")

# ---------------- Built-in sanity prompts (fallback) ----------------
DEFAULT_SANITY_PROMPTS = [
    "ŸÖÿß ŸáŸä ÿ≥ÿßÿπÿßÿ™ ÿßŸÑÿØŸàÿßŸÖ ÿßŸÑÿ±ÿ≥ŸÖŸäÿ© ŸÖŸÜ Ÿàÿ•ŸÑŸâÿü",
    "ŸáŸÑ ŸäŸàÿ¨ÿØ ŸÖÿ±ŸàŸÜÿ© ŸÅŸä ÿßŸÑÿ≠ÿ∂Ÿàÿ± ŸàÿßŸÑÿßŸÜÿµÿ±ÿßŸÅÿü ŸàŸÉŸäŸÅ ÿ™Ÿèÿ≠ÿ≥ÿ® ÿØŸÇÿßÿ¶ŸÇ ÿßŸÑÿ™ÿ£ÿÆŸäÿ±ÿü",
    "ŸáŸÑ ÿ™Ÿàÿ¨ÿØ ÿßÿ≥ÿ™ÿ±ÿßÿ≠ÿ© ÿÆŸÑÿßŸÑ ÿßŸÑÿØŸàÿßŸÖÿü ŸàŸÉŸÖ ŸÖÿØÿ™Ÿáÿßÿü",
    "ŸÖÿß ÿ≥ÿßÿπÿßÿ™ ÿßŸÑÿπŸÖŸÑ ŸÅŸä ÿ¥Ÿáÿ± ÿ±ŸÖÿ∂ÿßŸÜÿü ŸàŸáŸÑ ÿ™ÿ™ÿ∫Ÿäÿ±ÿü",
    "ŸÖÿß ÿ£ŸäÿßŸÖ ÿßŸÑÿØŸàÿßŸÖ ÿßŸÑÿ±ÿ≥ŸÖŸäÿü ŸàŸáŸÑ ÿßŸÑÿ≥ÿ®ÿ™ ŸäŸàŸÖ ÿπŸÖŸÑÿü",
    "ŸÉŸäŸÅ ŸäŸèÿ≠ÿ™ÿ≥ÿ® ÿßŸÑÿ£ÿ¨ÿ± ÿπŸÜ ÿßŸÑÿ≥ÿßÿπÿßÿ™ ÿßŸÑÿ•ÿ∂ÿßŸÅŸäÿ© ŸÅŸä ÿßŸÑÿ£ŸäÿßŸÖ ÿßŸÑÿπÿßÿØŸäÿ©ÿü",
    "ŸÖÿß ÿßŸÑÿ™ÿπŸàŸäÿ∂ ÿπŸÜÿØ ÿßŸÑÿπŸÖŸÑ ŸÅŸä ÿßŸÑÿπÿ∑ŸÑ ÿßŸÑÿ±ÿ≥ŸÖŸäÿ©ÿü",
    "ŸáŸÑ Ÿäÿ≠ÿ™ÿßÿ¨ ÿßŸÑÿπŸÖŸÑ ÿßŸÑÿ•ÿ∂ÿßŸÅŸä ŸÑŸÖŸàÿßŸÅŸÇÿ© ŸÖÿ≥ÿ®ŸÇÿ©ÿü ŸàŸÖŸÜ Ÿäÿπÿ™ŸÖÿØŸáÿßÿü",
    "ŸÉŸÖ ŸÖÿØÿ© ÿßŸÑÿ•ÿ¨ÿßÿ≤ÿ© ÿßŸÑÿ≥ŸÜŸàŸäÿ© ŸÑŸÖŸàÿ∏ŸÅ ÿ¨ÿØŸäÿØÿü ŸàŸÖÿ™Ÿâ ÿ™ÿ≤ŸäÿØÿü",
    "ŸáŸÑ ÿ™Ÿèÿ±ÿ≠ŸëŸÑ ÿßŸÑÿ•ÿ¨ÿßÿ≤ÿßÿ™ ÿ∫Ÿäÿ± ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖÿ©ÿü ŸàŸÖÿß ÿßŸÑÿ≠ÿØ ÿßŸÑÿ£ŸÇÿµŸâÿü",
    "ŸÖÿß ÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿ•ÿ¨ÿßÿ≤ÿ© ÿßŸÑÿ∑ÿßÿ±ÿ¶ÿ©ÿü ŸàŸÉŸäŸÅ ÿ£ÿ∑ŸÑÿ®Ÿáÿßÿü",
    "ŸÖÿß ÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿ•ÿ¨ÿßÿ≤ÿ© ÿßŸÑŸÖÿ±ÿ∂Ÿäÿ©ÿü ŸàÿπÿØÿØ ÿ£ŸäÿßŸÖŸáÿßÿü ŸàŸáŸÑ ŸäŸÑÿ≤ŸÖ ÿ™ŸÇÿ±Ÿäÿ± ÿ∑ÿ®Ÿäÿü",
    "ŸÉŸÖ ŸÖÿØÿ© ÿ•ÿ¨ÿßÿ≤ÿ© ÿßŸÑÿ£ŸÖŸàŸÖÿ©ÿü ŸàŸáŸÑ ŸäŸÖŸÉŸÜ ÿ£ÿÆÿ∞ ÿ¨ÿ≤ÿ° ŸÇÿ®ŸÑ ÿßŸÑŸàŸÑÿßÿØÿ©ÿü",
    "ŸÖÿß ŸáŸä ÿ•ÿ¨ÿßÿ≤ÿ© ÿßŸÑÿ≠ÿØÿßÿØÿü ŸÑŸÖŸÜ ÿ™ŸèŸÖŸÜÿ≠ ŸàŸÉŸÖ ŸÖÿØÿ™Ÿáÿßÿü",
    "ŸÖÿ™Ÿâ Ÿäÿ™ŸÖ ÿµÿ±ŸÅ ÿßŸÑÿ±Ÿàÿßÿ™ÿ® ÿ¥Ÿáÿ±ŸäŸãÿßÿü",
    "ŸÖÿß ŸáŸà ÿ®ÿØŸÑ ÿßŸÑŸÖŸàÿßÿµŸÑÿßÿ™ÿü ŸàŸáŸÑ Ÿäÿ¥ŸÖŸÑ ÿßŸÑÿ∞Ÿáÿßÿ® ŸÖŸÜ ÿßŸÑŸÖŸÜÿ≤ŸÑ ŸÑŸÑÿπŸÖŸÑÿü ŸàŸÉŸäŸÅ ŸäŸèÿµÿ±ŸÅÿü",
    "ŸáŸÑ ÿ™Ÿàÿ¨ÿØ ÿ≥ŸÑŸÅ ÿπŸÑŸâ ÿßŸÑÿ±ÿßÿ™ÿ®ÿü ŸàŸÖÿß ÿ¥ÿ±Ÿàÿ∑Ÿáÿßÿü",
    "ŸÖÿß ÿßŸÑÿ≠ÿØ ÿßŸÑÿ£ŸÇÿµŸâ ŸÑŸÑŸÜÿ´ÿ±Ÿäÿßÿ™ ÿßŸÑŸäŸàŸÖŸäÿ©ÿü ŸàŸÉŸäŸÅ ÿ™ÿ™ŸÖ ÿßŸÑÿ™ÿ≥ŸàŸäÿ© ŸàÿßŸÑŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿ©ÿü",
    "ŸÖÿß ÿ≥ŸÇŸÅ ÿßŸÑÿ¥ÿ±ÿßÿ° ÿßŸÑÿ∞Ÿä Ÿäÿ≥ÿ™ŸÑÿ≤ŸÖ ÿ´ŸÑÿßÿ´ÿ© ÿπÿ±Ÿàÿ∂ ÿ£ÿ≥ÿπÿßÿ±ÿü",
    "ŸÖÿß ÿ∂Ÿàÿßÿ®ÿ∑ ÿ™ÿ∂ÿßÿ±ÿ® ÿßŸÑŸÖÿµÿßŸÑÿ≠ ŸÅŸä ÿßŸÑŸÖÿ¥ÿ™ÿ±Ÿäÿßÿ™ÿü",
    "ŸÖÿß ÿ≠ÿØŸàÿØ ŸÇÿ®ŸàŸÑ ÿßŸÑŸáÿØÿßŸäÿß ŸàÿßŸÑÿ∂ŸäÿßŸÅÿ©ÿü ŸàŸÖÿ™Ÿâ Ÿäÿ¨ÿ® ÿßŸÑÿ•ÿ®ŸÑÿßÿ∫ÿü",
    "ŸÉŸäŸÅ ÿ£ÿ≥ÿ™ŸÑŸÖ ÿπŸáÿØÿ© ÿ¨ÿØŸäÿØÿ©ÿü ŸàŸÖÿß ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßŸÑŸÖÿ∑ŸÑŸàÿ®ÿü",
    "ŸÉŸäŸÅ ÿ£ÿ≥ŸÑŸëŸÖ ÿßŸÑÿπŸáÿØÿ© ÿπŸÜÿØ ÿßŸÑÿßÿ≥ÿ™ŸÇÿßŸÑÿ© ÿ£Ÿà ÿßŸÑÿßŸÜÿ™ŸÇÿßŸÑÿü",
    "ŸÖÿß ÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿπŸÖŸÑ ÿπŸÜ ÿ®ŸèÿπÿØ/ŸÖŸÜ ÿßŸÑŸÖŸÜÿ≤ŸÑÿü ŸàŸÉŸäŸÅ Ÿäÿ™ŸÖ ÿßÿπÿ™ŸÖÿßÿØŸáÿü",
    "ŸÉŸäŸÅ ÿ£ŸÇÿØŸëŸÖ ÿ•ÿ∞ŸÜ ŸÖÿ∫ÿßÿØÿ±ÿ© ÿ≥ÿßÿπŸäÿ©ÿü ŸàŸÖÿß ÿßŸÑÿ≠ÿØ ÿßŸÑÿ£ŸÇÿµŸâ ÿßŸÑÿ¥Ÿáÿ±Ÿäÿü",
    "ŸÖÿ™Ÿâ Ÿäÿ™ŸÖ ÿ™ŸÇŸäŸäŸÖ ÿßŸÑÿ£ÿØÿßÿ° ÿßŸÑÿ≥ŸÜŸàŸäÿü ŸàŸÖÿß ŸÖÿπÿßŸäŸäÿ±Ÿá ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©ÿü",
    "ŸÖÿß ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑÿ•ŸÜÿ∞ÿßÿ± ŸàÿßŸÑÿ™ÿØÿ±Ÿëÿ¨ ÿßŸÑÿ™ÿ£ÿØŸäÿ®Ÿä ŸÑŸÑŸÖÿÆÿßŸÑŸÅÿßÿ™ÿü",
    "ŸÖÿß ÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿ≥ÿ±Ÿäÿ© Ÿàÿ≠ŸÖÿßŸäÿ© ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ÿü",
    "ŸÖÿß ÿ≥Ÿäÿßÿ≥ÿ© ÿßŸÑÿ≥ŸÑŸàŸÉ ÿßŸÑŸÖŸáŸÜŸä ŸàŸÖŸÉÿßŸÅÿ≠ÿ© ÿßŸÑÿ™ÿ≠ÿ±ÿ¥ÿü",
    "ŸáŸÑ ÿ™Ÿàÿ¨ÿØ ŸÖŸäÿßŸàŸÖÿßÿ™/ÿ®ÿØŸÑ ÿ≥ŸÅÿ±ÿü ŸàŸÉŸäŸÅ ÿ™Ÿèÿµÿ±ŸÅ",
]

# ---------------- Answer Handler (Minimal interference) ----------------
def ask_once(index: RET.HybridIndex,
             tokenizer,
             model,
             question: str,
             use_llm: bool = True) -> str:
    """
    One Q&A round:
      1) classify intent via RET.classify_intent
      2) get extractive answer via RET.answer (includes sources)
      3) optional: refine wording with LLM, preserving sources
    """
    t0 = time.time()
    intent = RET.classify_intent(question)

    # Retrieval-first
    extractive_answer = RET.answer(question, index, intent, use_rerank_flag=True)

    # If LLM disabled/unavailable ‚Üí return extractive
    if not use_llm or tokenizer is None or model is None:
        dt = time.time() - t0
        if isinstance(extractive_answer, str) and extractive_answer.startswith("‚è±"):
            return extractive_answer
        return f"‚è± {dt:.2f}s | ü§ñ {extractive_answer}"

    # For work-hours style outputs that are already clean, don't over-process
    if intent in ("work_hours", "ramadan_hours") and ("ÿ≥ÿßÿπÿßÿ™ ÿßŸÑÿØŸàÿßŸÖ" in extractive_answer) and ("ŸÖŸÜ" in extractive_answer) and ("ÿ•ŸÑŸâ" in extractive_answer):
        dt = time.time() - t0
        if extractive_answer.startswith("‚è±"):
            return extractive_answer
        return f"‚è± {dt:.2f}s | ü§ñ {extractive_answer}"

    # Split body/sources to preserve citations
    lines = extractive_answer.split('\n')
    body_lines, source_lines = [], []
    sources_started = False
    for line in lines:
        ls = line.strip()
        if ls.startswith("Sources:") or ls.startswith("ÿßŸÑŸÖÿµÿßÿØÿ±:"):
            sources_started = True
            source_lines.append(line)
        elif sources_started:
            source_lines.append(line)
        else:
            body_lines.append(line)

    body = '\n'.join(body_lines).strip()
    sources = '\n'.join(source_lines).strip()

    # If retrieval failed, skip LLM
    if not body or "ŸÑŸÖ ÿ£ÿπÿ´ÿ±" in body or "ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿπŸÑŸàŸÖÿßÿ™" in body:
        dt = time.time() - t0
        if extractive_answer.startswith("‚è±"):
            return extractive_answer
        return f"‚è± {dt:.2f}s | ü§ñ {extractive_answer}"

    # LLM refinement (Arabic)
    try:
        from transformers import PreTrainedTokenizerBase
        system_prompt = "ÿ£ÿπÿØ ÿµŸäÿßÿ∫ÿ© ÿßŸÑÿ•ÿ¨ÿßÿ®ÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ© ÿ®ÿ¥ŸÉŸÑ Ÿàÿßÿ∂ÿ≠ ŸàŸÖÿÆÿ™ÿµÿ± ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©:"
        user_prompt = f"ÿßŸÑÿ≥ÿ§ÿßŸÑ: {question}\nÿßŸÑÿ•ÿ¨ÿßÿ®ÿ©: {body}"

        msgs = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        # Not all tokenizers expose apply_chat_template
        if hasattr(tokenizer, "apply_chat_template"):
            prompt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
        else:
            prompt = f"[system]\n{system_prompt}\n\n[user]\n{user_prompt}\n\n[assistant]\n"

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        # Move to device
        if hasattr(model, "device"):
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

        eos_id = getattr(tokenizer, "eos_token_id", None)
        pad_id = eos_id if eos_id is not None else getattr(tokenizer, "pad_token_id", None)

        out_ids = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.1,
            do_sample=False,
            eos_token_id=eos_id,
            pad_token_id=pad_id,
        )
        start = inputs['input_ids'].shape[1]
        resp = tokenizer.decode(out_ids[0][start:], skip_special_tokens=True).strip()
        # Keep the first line concise
        resp_line = resp.split('\n')[0].strip() if resp else ""

        dt = time.time() - t0
        if resp_line and len(resp_line) > 5:
            return f"‚è± {dt:.2f}s | ü§ñ {resp_line}\n{sources}" if sources else f"‚è± {dt:.2f}s | ü§ñ {resp_line}"
    except Exception as e:
        LOG.warning(f"LLM generation failed: {e}")

    # Fallback: extractive
    dt = time.time() - t0
    if extractive_answer.startswith("‚è±"):
        return extractive_answer
    return f"‚è± {dt:.2f}s | ü§ñ {extractive_answer}"


def _gather_sanity_prompts() -> list:
    """Merge RET.SANITY_PROMPTS (if any) with DEFAULT_SANITY_PROMPTS, preserving order and uniqueness."""
    ret_prompts = []
    try:
        ret_prompts = list(getattr(RET, "SANITY_PROMPTS", []) or [])
    except Exception:
        ret_prompts = []
    seen = set()
    merged = []
    for q in (ret_prompts + DEFAULT_SANITY_PROMPTS):
        if q not in seen:
            seen.add(q)
            merged.append(q)
    return merged


def run_test_prompts(index: RET.HybridIndex, tokenizer, model, use_llm: bool):
    """
    Run sanity prompts (merged list). PASS heuristic: includes 'Sources:' and not a generic fail string.
    """
    test_prompts = _gather_sanity_prompts()
    if not test_prompts:
        print("‚ùå No sanity prompts available.")
        return

    print("üß™ Running sanity prompts ...")
    print("=" * 80)

    passed = 0
    total = len(test_prompts)
    for i, q in enumerate(test_prompts, 1):
        print(f"\nüìù Test {i}/{total}: {q}")
        print("-" * 60)
        try:
            result = ask_once(index, tokenizer, model, q, use_llm=use_llm)
            print(result)
            ok = ("Sources:" in result or "ÿßŸÑŸÖÿµÿßÿØÿ±:" in result) and ("ŸÑŸÖ ÿ£ÿπÿ´ÿ±" not in result) and ("ŸÑÿß ÿ™Ÿàÿ¨ÿØ ŸÖÿπŸÑŸàŸÖÿßÿ™" not in result)
            print("‚úÖ PASS" if ok else "‚ùå FAIL")
            passed += int(ok)
        except Exception as e:
            print(f"‚ùå Error: {e}")
        print("=" * 80)

    print(f"\nSummary: PASS {passed}/{total}")


# ---------------- CLI ----------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--chunks", type=str, default="Data_pdf_clean_chunks.jsonl",
                    help="Path to chunks (JSONL or JSON) used by retriever")
    ap.add_argument("--hier-index", type=str, default="heading_inverted_index.json",
                    help="Optional hierarchical inverted index path")
    ap.add_argument("--aliases", type=str, default="section_aliases.json",
                    help="Optional section aliases path")
    ap.add_argument("--save-index", type=str, default=None,
                    help="Directory to save index artifacts (embeddings/FAISS/TF-IDF)")
    ap.add_argument("--load-index", type=str, default=None,
                    help="Directory to load index artifacts from")
    ap.add_argument("--model", type=str, default="Qwen/Qwen2.5-7B-Instruct",
                    help="HF model id for optional refinement")
    ap.add_argument("--ask", type=str, default=None,
                    help="Ask a single question then exit")
    ap.add_argument("--test", action="store_true",
                    help="Run sanity prompts (alias: --sanity)")
    ap.add_argument("--sanity", action="store_true",
                    help="Alias for --test (runs sanity prompts)")
    ap.add_argument("--no-llm", action="store_true",
                    help="Disable LLM refinement (retrieval-only)")
    ap.add_argument("--use-4bit", action="store_true",
                    help="Try 4-bit quantization (requires bitsandbytes)")
    ap.add_argument("--use-8bit", action="store_true",
                    help="Try 8-bit quantization (requires bitsandbytes)")
    args = ap.parse_args()

    # Build/load index via RET
    hier = RET.load_hierarchy(args.hier_index, args.aliases)

    if not os.path.exists(args.chunks):
        LOG.error("Chunks file not found: %s", args.chunks)
        return

    chunks, chunks_hash = RET.load_chunks(path=args.chunks)
    index = RET.HybridIndex(chunks, chunks_hash, hier=hier)

    loaded = False
    if args.load_index and os.path.exists(args.load_index):
        try:
            # Reduce noise while loading
            retrieval_logger = logging.getLogger("retrival_model")
            original_level = retrieval_logger.level
            retrieval_logger.setLevel(logging.ERROR)

            loaded = index.load(args.load_index)

            retrieval_logger.setLevel(original_level)
            if loaded:
                LOG.info("Index loaded successfully from %s", args.load_index)
        except Exception as e:
            LOG.info(f"Will rebuild index: {e}")

    if not loaded:
        LOG.info("Building index ...")
        index.build()
        if args.save_index:
            try:
                index.save(args.save_index)
                LOG.info("Index saved to %s", args.save_index)
            except Exception as e:
                LOG.warning(f"Failed to save index: {e}")

    # Optional LLM
    tok = mdl = None
    use_llm = not args.no_llm
    if use_llm:
        try:
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

            # Default dtype/device
            bf16_supported = False
            if torch is not None and hasattr(torch, "cuda") and torch.cuda.is_available():
                bf16_supported = getattr(torch.cuda, "is_bf16_supported", lambda: False)()
            dtype_fp16 = None
            if torch is not None:
                dtype_fp16 = torch.bfloat16 if bf16_supported else torch.float16

            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": dtype_fp16 or None,
            }

            if torch is not None and hasattr(torch, "cuda") and torch.cuda.is_available():
                model_kwargs["device_map"] = "auto"
            else:
                model_kwargs["device_map"] = "cpu"
                if torch is not None:
                    model_kwargs["torch_dtype"] = torch.float32

            if args.use_4bit or args.use_8bit:
                try:
                    quant_config = BitsAndBytesConfig(
                        load_in_4bit=True if args.use_4bit else False,
                        load_in_8bit=True if args.use_8bit else False,
                        bnb_4bit_compute_dtype=(torch.bfloat16 if (torch is not None and bf16_supported) else (torch.float16 if torch is not None else None)),
                    )
                    model_kwargs["quantization_config"] = quant_config
                except Exception as e:
                    LOG.warning(f"Quantization setup failed: {e}")

            tok = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)
            mdl = AutoModelForCausalLM.from_pretrained(args.model, **model_kwargs)
        except Exception as e:
            LOG.warning(f"Failed to load LLM (%s); continuing retrieval-only. Error: %s", args.model, e)
            tok = mdl = None
            use_llm = False

    # Run sanity prompts (alias: --sanity)
    if args.test or args.sanity:
        run_test_prompts(index, tok, mdl, use_llm=use_llm)
        return

    # Single-question mode
    if args.ask:
        print(ask_once(index, tok, mdl, args.ask, use_llm=use_llm))
        return

    # Interactive loop
    print("Ready. ÿßÿ∑ÿ±ÿ≠ ÿ≥ÿ§ÿßŸÑŸÉ (ÿßŸÉÿ™ÿ® 'exit' ŸÑŸÑÿÆÿ±Ÿàÿ¨)\n")
    while True:
        try:
            q = input("ÿ≥ÿ§ÿßŸÑŸÉ: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nExiting.")
            break
        if not q:
            continue
        if q.lower() in ("exit", "quit", "q"):
            print("Exiting.")
            break
        print(ask_once(index, tok, mdl, q, use_llm=use_llm))


if __name__ == "__main__":
    main()
