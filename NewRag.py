# ==================== PATCH FOR BITSANDBYTES METADATA ISSUE ====================
import importlib.metadata
_original_version = importlib.metadata.version
def patched_version(package_name: str) -> str:
    if package_name == "bitsandbytes":
        return "0.47.0"  # set to your installed version
    return _original_version(package_name)
importlib.metadata.version = patched_version
# ==============================================================================

import os, re, json, time, pickle, warnings, hashlib
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional, Any, Set
from pathlib import Path

import numpy as np
warnings.filterwarnings("ignore")

# ---------------- Optional deps ----------------
try:
    import faiss  # faiss-cpu
except Exception:
    faiss = None

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    import joblib
except Exception:
    TfidfVectorizer = None
    joblib = None

from sentence_transformers import SentenceTransformer

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)

# ======================= Arabic utilities =======================
AR_DIAC = re.compile(r'[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED]')
AR_NUMS = {ord(c): ord('0')+i for i,c in enumerate(u"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©")}
IR_NUMS = {ord(c): ord('0')+i for i,c in enumerate(u"Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹")}
SENT_SPLIT_RE = re.compile(r'(?<=[\.\!\ØŸ\?ØŒØ›]|[\nâ€”])\s+')

def ar_normalize(s: str) -> str:
    if not s: return ""
    s = s.replace('\u0640','')                       # tatweel
    s = AR_DIAC.sub('', s)                           # diacritics
    s = (s.replace('Ø£','Ø§').replace('Ø¥','Ø§').replace('Ø¢','Ø§')
           .replace('Ù‰','ÙŠ').replace('Ø©','Ù‡'))       # unify alif/ya/ta marbuta
    s = s.translate(AR_NUMS).translate(IR_NUMS)      # Arabic/Indic digits -> ASCII
    s = s.replace('ØŒ', ',').replace('Ù«','.')
    s = ' '.join(s.split())
    return s

def rtl_wrap(t: str) -> str:
    return '\u202B' + t + '\u202C'

def sent_split(s: str) -> List[str]:
    if not s: return []
    parts = [p.strip() for p in SENT_SPLIT_RE.split(s) if p and p.strip()]
    out = []
    for p in parts:
        pn = ar_normalize(p)
        if len(pn) < 6:  # drop tiny
            continue
        letters = sum(ch.isalpha() for ch in pn)
        total = len(pn.replace(" ", ""))
        if total > 0 and letters/total < 0.5:  # drop noisy OCR-ish lines
            continue
        out.append(p)
    return out if out else ([s.strip()] if s.strip() else [])

# ======================= Data model =======================
@dataclass
class DocumentChunk:
    """Represents a document chunk with metadata"""
    id: str
    text_display: str
    text_embed: str
    language: str
    source: str
    page: int
    chunk_no: int
    embedding: Optional[np.ndarray] = None

# ======================= Hybrid Arabic Retriever =======================
class HybridArabicRetriever:
    """
    - Dense: SentenceTransformer + (FAISS IP) or NumPy fallback
    - Sparse: TF-IDF (char 2â€“5 + word 1â€“2), automatically disabled if sklearn not available
    - Index persistence: embeddings.npy, faiss.index, tfidf pickles + meta.json in artifact_dir
    - Input format: JSONL rows containing at least: id, text_display or text_embed, language, source, page, chunk_no
    """

    def __init__(
        self,
        model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        artifact_dir: str = "./artifacts"
    ):
        self.model_name = model_name
        print("ğŸ”„ Loading embedding modelâ€¦")
        t0 = time.time()
        self.model = SentenceTransformer(model_name)
        self.model.max_seq_length = 256
        print(f"âœ… Embedding model loaded in {time.time()-t0:.2f}s")

        self.chunks: List[DocumentChunk] = []
        self.emb: Optional[np.ndarray] = None
        self.faiss_index = None

        self.tf_char = None; self.tf_word = None
        self.char_mat = None; self.word_mat = None

        self.artifacts = Path(artifact_dir)
        self.artifacts.mkdir(parents=True, exist_ok=True)

    # ---------- dataset I/O ----------
    def load_jsonl(self, path: str) -> List[Dict]:
        data = []
        with open(path, "r", encoding="utf-8") as f:
            for i, line in enumerate(f, 1):
                line = line.strip()
                if not line: continue
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    if i <= 10:
                        print(f"âš ï¸ JSON error at line {i}: {e}")
        print(f"âœ… Loaded {len(data)} raw rows from {path}")
        return data

    def load_documents(self, rows: List[Dict]) -> None:
        """Use provided chunk fields; normalize text_embed if missing."""
        self.chunks.clear()
        for i, item in enumerate(rows):
            disp = item.get("text_display") or item.get("text") or ""
            embtxt = item.get("text_embed") or disp
            embtxt = ar_normalize(embtxt)
            if not embtxt: continue
            self.chunks.append(
                DocumentChunk(
                    id=str(item.get("id", f"row{i}")),
                    text_display=(disp or embtxt).strip(),
                    text_embed=embtxt,
                    language=item.get("language","ar"),
                    source=item.get("source","unknown"),
                    page=int(item.get("page",0)),
                    chunk_no=int(item.get("chunk_no",0))
                )
            )
        print(f"âœ… Prepared {len(self.chunks)} chunks")

    # ---------- persistence helpers ----------
    def _dataset_hash(self) -> str:
        h = hashlib.sha256()
        for c in self.chunks[:200]:
            h.update(c.id.encode("utf-8"))
            h.update(c.text_embed[:256].encode("utf-8"))
        return h.hexdigest()[:16]

    def _p(self, name: str) -> Path:
        return self.artifacts / name

    def build_or_load(self, use_cache: bool = True, batch_size: int = 128) -> None:
        if not self.chunks:
            raise ValueError("No documents loaded.")
        ds_hash = self._dataset_hash()
        meta_path = self._p("meta.json")
        emb_path  = self._p("embeddings.npy")
        faiss_path= self._p("faiss.index")
        tfc_path  = self._p("tf_char.pkl")
        tfw_path  = self._p("tf_word.pkl")
        cm_path   = self._p("char_mat.pkl")
        wm_path   = self._p("word_mat.pkl")

        # try load
        if use_cache and meta_path.exists():
            try:
                meta = json.loads(meta_path.read_text(encoding="utf-8"))
                cond = (meta.get("ds_hash")==ds_hash and
                        meta.get("model_name")==self.model_name and
                        meta.get("n_chunks")==len(self.chunks))
                if cond and emb_path.exists():
                    print("ğŸ“¦ Loading dense artifactsâ€¦")
                    self.emb = np.load(str(emb_path), mmap_mode="r")
                    if faiss is not None and faiss_path.exists():
                        self.faiss_index = faiss.read_index(str(faiss_path))
                    elif faiss is not None:
                        d = self.emb.shape[1]
                        self.faiss_index = faiss.IndexFlatIP(d); self.faiss_index.add(self.emb.astype("float32"))
                    if TfidfVectorizer and joblib and tfc_path.exists() and tfw_path.exists() and cm_path.exists() and wm_path.exists():
                        print("ğŸ“¦ Loading TF-IDF artifactsâ€¦")
                        self.tf_char = joblib.load(str(tfc_path))
                        self.tf_word = joblib.load(str(tfw_path))
                        self.char_mat = joblib.load(str(cm_path))
                        self.word_mat = joblib.load(str(wm_path))
                    print("âœ… Index loaded from artifacts")
                    return
            except Exception as e:
                print(f"âš ï¸ Artifact load failed, rebuildingâ€¦ ({e})")

        # build
        print("ğŸ”„ Building embeddings + indexesâ€¦")
        t0 = time.time()
        texts = [c.text_embed for c in self.chunks]
        self.emb = self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)

        if faiss is not None:
            d = self.emb.shape[1]
            if len(self.emb) < 10_000:
                self.faiss_index = faiss.IndexFlatIP(d)
                self.faiss_index.add(self.emb.astype("float32"))
            else:
                nlist = min(100, len(self.emb) // 100)
                quantizer = faiss.IndexFlatIP(d)
                self.faiss_index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_INNER_PRODUCT)
                self.faiss_index.train(self.emb.astype("float32"))
                self.faiss_index.add(self.emb.astype("float32"))
        else:
            self.faiss_index = None  # fallback to NumPy

        if TfidfVectorizer:
            print("ğŸ§ª Building TF-IDF (char+word)â€¦")
            self.tf_char = TfidfVectorizer(analyzer="char", ngram_range=(2,5), min_df=1)
            self.char_mat = self.tf_char.fit_transform(texts)
            self.tf_word = TfidfVectorizer(analyzer="word", ngram_range=(1,2), token_pattern=r"(?u)\b\w+\b", min_df=1)
            self.word_mat = self.tf_word.fit_transform(texts)
        else:
            print("â„¹ï¸ scikit-learn not available â€” sparse channel disabled")

        # persist
        print("ğŸ’¾ Saving artifactsâ€¦")
        try:
            np.save(str(emb_path), self.emb)
            if faiss is not None and self.faiss_index is not None:
                faiss.write_index(self.faiss_index, str(faiss_path))
            if TfidfVectorizer and joblib:
                joblib.dump(self.tf_char, str(tfc_path)); joblib.dump(self.tf_word, str(tfw_path))
                joblib.dump(self.char_mat, str(cm_path)); joblib.dump(self.word_mat, str(wm_path))
            meta = {"ds_hash": ds_hash, "model_name": self.model_name, "n_chunks": len(self.chunks)}
            meta_path.write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")
            print(f"âœ… Artifacts saved to {self.artifacts}")
        except Exception as e:
            print(f"âš ï¸ Failed to save artifacts: {e}")

        print(f"âœ… Index build complete in {time.time()-t0:.2f}s")

    # ---------- retrieval ----------
    def _dense_search(self, q: str, topk: int = 20):
        qn = ar_normalize(q)
        qv = self.model.encode([qn], convert_to_numpy=True, normalize_embeddings=True)
        if self.faiss_index is not None:
            D, I = self.faiss_index.search(qv.astype("float32"), topk)
            return D[0], I[0]
        # NumPy fallback
        sims = self.emb @ qv[0]
        idxs = np.argsort(-sims)[:topk]
        return sims[idxs], idxs

    def _sparse_scores(self, q: str):
        if self.tf_char is None or self.tf_word is None:
            return None, None
        qc = self.tf_char.transform([ar_normalize(q)])
        qw = self.tf_word.transform([ar_normalize(q)])
        c_scores = (self.char_mat @ qc.T).toarray().ravel()
        w_scores = (self.word_mat @ qw.T).toarray().ravel()
        return c_scores, w_scores

    def _combine(self, dense_scores, dense_idx, c_scores, w_scores,
                 w_dense=0.65, w_char=0.20, w_word=0.15, topk=10):
        out = []
        for s, i in zip(dense_scores, dense_idx):
            sc = float(s) * w_dense
            if c_scores is not None and len(c_scores) > int(i): sc += float(c_scores[i]) * w_char
            if w_scores is not None and len(w_scores) > int(i): sc += float(w_scores[i]) * w_word
            out.append((sc, int(i)))
        out.sort(key=lambda x: -x[0])
        return out[:topk]

    def semantic_search(self, query: str, top_k: int = 5) -> List[Tuple[DocumentChunk, float]]:
        dS, dI = self._dense_search(query, topk=max(top_k*4, 20))
        cS, wS = self._sparse_scores(query)
        combined = self._combine(dS, dI, cS, wS, topk=top_k)
        results = []
        for sc, idx in combined:
            if 0 <= idx < len(self.chunks):
                results.append((self.chunks[idx], float(sc)))
        return results

    def get_relevant_context(self, query: str, max_context_length: int = 1500, top_k: int = 5) -> Tuple[str, List[Dict]]:
        hits = self.semantic_search(query, top_k=top_k)
        if not hits: return "", []
        parts, meta, total = [], [], 0
        for rank, (chunk, score) in enumerate(hits, 1):
            head = f"[Ø§Ù„Ù…ØµØ¯Ø± {rank}: {chunk.source} - Øµ{chunk.page}]"
            body = chunk.text_display.strip()
            needed = len(head)+1+len(body)+2
            if total + needed <= max_context_length:
                parts.append(f"{head}\n{body}")
                total += needed
                meta.append({"source": chunk.source, "page": chunk.page, "score": score, "chunk_id": chunk.id})
            else:
                remain = max_context_length - total - len(head) - 2
                if remain > 120:
                    trunc = body[:remain-20] + " â€¦"
                    parts.append(f"{head}\n{trunc}")
                    meta.append({"source": chunk.source, "page": chunk.page, "score": score, "chunk_id": chunk.id, "truncated": True})
                break
        return "\n\n".join(parts), meta

# ======================= Optimized SambaLingo RAG =======================
class OptimizedSambaLingoRAG:
    """
    Fast, Arabic-first RAG:
      - Hybrid retriever (dense + TF-IDF) with caching
      - SambaLingo Arabic chat model (4-bit NF4 by default)
      - Short, focused prompt (keeps latency low)
    """

    def __init__(self, jsonl_filepath: str, artifact_dir: str = "./artifacts", use_cache: bool = True):
        print("ğŸš€ Initializing Optimized Arabic RAG Systemâ€¦")
        t0 = time.time()

        print("ğŸ“š Setting up hybrid retrieverâ€¦")
        self.retriever = HybridArabicRetriever(artifact_dir=artifact_dir)
        rows = self._load_jsonl_fast(jsonl_filepath)
        if not rows:
            raise ValueError("No data loaded from JSONL file")
        self.retriever.load_documents(rows)
        self.retriever.build_or_load(use_cache=use_cache, batch_size=128)

        print("ğŸ§  Loading SambaLingo Arabic modelâ€¦")
        self.tokenizer, self.model = self._setup_sambanova_model()

        print(f"âœ… RAG System initialized in {time.time()-t0:.2f}s")

    # ---------- data loader ----------
    def _load_jsonl_fast(self, filepath: str) -> List[Dict]:
        data = []
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            print(f"ğŸ“– Processing {len(lines)} linesâ€¦")
            for i, line in enumerate(lines, 1):
                line = line.strip()
                if not line: continue
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    if i <= 10:
                        print(f"âš ï¸ Error parsing line {i}: {e}")
                if i % 5000 == 0:
                    print(f"  Processed {i}/{len(lines)}")
            print(f"âœ… Loaded {len(data)} valid chunks")
        except FileNotFoundError:
            print(f"âŒ File not found: {filepath}")
        return data

    # ---------- model ----------
    def _setup_sambanova_model(self):
        model_name = "sambanovasystems/SambaLingo-Arabic-Chat"
        # quant config (good on RTX 4060+)
        try:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                llm_int8_enable_fp32_cpu_offload=True
            )
        except Exception:
            bnb_config = None

        tok = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side="left"
        )
        if tok.pad_token is None:
            tok.pad_token = tok.eos_token

        kwargs = dict(
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            use_cache=True
        )
        if bnb_config is not None:
            kwargs["quantization_config"] = bnb_config
            kwargs["torch_dtype"] = torch.float16
        else:
            kwargs["torch_dtype"] = torch.float16 if torch.cuda.is_available() else torch.bfloat16

        model = AutoModelForCausalLM.from_pretrained(model_name, **kwargs)
        model.eval()
        if hasattr(model, "gradient_checkpointing_enable"):
            model.gradient_checkpointing_enable()

        try:
            if torch.__version__ >= "2.0":
                model = torch.compile(model, mode="reduce-overhead")
                print("ğŸš€ Model compiled for faster inference")
        except Exception as e:
            print(f"â„¹ï¸ Compile skipped: {e}")

        print(f"ğŸ“Š Model on: {model.device}")
        return tok, model

    # ---------- prompt ----------
    def _make_prompt(self, question: str, context: str) -> str:
        return f"""<|start_header_id|>system<|end_header_id|>
Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ø¹ØªÙ…Ø§Ø¯Ø§Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ§Ù„ÙŠ. ÙƒÙ† Ù…Ø®ØªØµØ±Ø§Ù‹ ÙˆØ¯Ù‚ÙŠÙ‚Ø§Ù‹ØŒ ÙˆØ¥Ù† ØºØ§Ø¨ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙØ§Ø°ÙƒØ± Ø°Ù„Ùƒ ØµØ±Ø§Ø­Ø©Ù‹.<|end_header_id|>

<|start_header_id|>user<|end_header_id|>
Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {question}<|end_header_id|>

<|start_header_id|>assistant<|end_header_id|>"""

    # ---------- generate ----------
    def generate_answer(self, question: str, max_new_tokens: int = 256, temperature: float = 0.3) -> Dict:
        t0 = time.time()
        print("ğŸ” Retrieving contextâ€¦")
        context, meta = self.retriever.get_relevant_context(question, max_context_length=1200, top_k=5)
        if not context.strip():
            return {
                "answer": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£Ø¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.",
                "context": "",
                "metadata": [],
                "confidence": 0.0,
                "time_taken": time.time()-t0
            }

        prompt = self._make_prompt(question, context)
        print("ğŸ§  Generating answerâ€¦")
        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=1500,
                padding=False
            ).to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=temperature > 0,
                    top_p=0.85,
                    repetition_penalty=1.08,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True,
                    use_cache=True
                )
            text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            # extract assistant tail
            if "<|start_header_id|>assistant<|end_header_id|>" in text:
                answer = text.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
            else:
                answer = text[len(prompt):].strip()
            answer = answer.replace("<|end_header_id|>", "").strip()

            return {
                "answer": answer,
                "context": context,
                "metadata": meta,
                "confidence": 0.8,   # simple heuristic baseline
                "time_taken": time.time()-t0
            }
        except Exception as e:
            print(f"âŒ Generation error: {e}")
            return {
                "answer": "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©.",
                "context": context,
                "metadata": meta,
                "confidence": 0.0,
                "time_taken": time.time()-t0
            }

    # ---------- helpers ----------
    def ask(self, question: str) -> str:
        return self.generate_answer(question)["answer"]

    def benchmark(self, questions: List[str]) -> None:
        print("\nğŸ§ª Benchmarkâ€¦")
        total = 0.0
        for i, q in enumerate(questions, 1):
            print(f"\n{i}. {q}")
            r = self.generate_answer(q)
            print(f"â± {r['time_taken']:.2f}s | ğŸ¤– {r['answer'][:200]}{'â€¦' if len(r['answer'])>200 else ''}")
            total += r["time_taken"]
        n = max(len(questions), 1)
        print(f"\nğŸ“Š Avg time: {total/n:.2f}s | Total: {total:.2f}s")

    def chat_loop(self):
        print("\n" + "="*60)
        print("ğŸ¤– Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† (SambaLingo + Hybrid RAG)")
        print("ğŸ’¡ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø§Ù†ØªÙ‡Ø§Ø¡")
        print("="*60 + "\n")
        while True:
            try:
                q = input("ğŸ™‹ Ø³Ø¤Ø§Ù„Ùƒ: ").strip()
                if not q: continue
                if q.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
                    print("ğŸ™ Ø´ÙƒØ±Ø§Ù‹!")
                    break
                r = self.generate_answer(q)
                print(f"\nğŸ¤– {r['answer']}\n")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ØªÙ… Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù"); break
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£: {e}")

    def cleanup(self):
        print("ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©â€¦")
        if hasattr(self, "model"): del self.model
        if hasattr(self, "tokenizer"): del self.tokenizer
        if hasattr(self, "retriever"): del self.retriever
        torch.cuda.empty_cache()
        print("âœ… ØªÙ…")

# ======================= Main =======================
if __name__ == "__main__":
    try:
        kb_path = os.environ.get("AR_KB_JSONL", "arabic_chatbot_knowledge.jsonl")
        rag = OptimizedSambaLingoRAG(kb_path, artifact_dir="./artifacts", use_cache=True)

        # quick bench
        test_qs = [
            "Ù…Ø§ Ù‡ÙŠ Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„ØªÙˆØ¸ÙŠÙ ÙÙŠ Ù…Ø¤Ø³Ø³Ø© Ø§Ù„Ù†ÙŠØ²ÙƒØŸ",
            "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø§Ù„ØªÙ‚Ø¯Ù… Ù„Ù„ÙˆØ¸Ø§Ø¦ÙØŸ",
            "Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„Ù…ÙˆØ¸Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯ØŸ"
        ]
        rag.benchmark(test_qs)

        # debug: show top hits example
        print("\nğŸ” Debug search: 'Ø§Ù„ØªÙˆØ¸ÙŠÙ'")
        hits = rag.retriever.semantic_search("Ø§Ù„ØªÙˆØ¸ÙŠÙ", top_k=3)
        for i, (chunk, score) in enumerate(hits, 1):
            print(f"  {i}. score={score:.3f} | src={chunk.source} | p={chunk.page} | text={chunk.text_display[:80]}â€¦")

        # start chat
        print("\nğŸ’¬ Starting chatâ€¦")
        rag.chat_loop()

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù†Ø¸Ø§Ù…")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù…: {e}")
        import traceback; traceback.print_exc()
    finally:
        if 'rag' in locals():
            rag.cleanup()
